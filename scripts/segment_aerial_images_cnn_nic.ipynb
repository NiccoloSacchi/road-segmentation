{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from helpers import *\n",
    "\n",
    "# import 'Sequential' is a linear stack of neural network layers. Will be used to build the feed-forward CNN\n",
    "from keras.models import Sequential \n",
    "# import the \"core\" layers from Keras (these are the most common layers)\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# import the convolutional layers that will help us efficiently train on image data\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# these utilities will help us transform our data\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "**Convolutional neural networks** (CNNs) are used to classify images. Owever our task does not consist in classifying the whole image but only its patches of 16x16 pixels. Therefore, we will need to properly choose the input of our CNN, i.e. it will have (at least) one input per patch so to classify each patch either as **road** or as **backgrund**. Moreover, each input shouldn't be just a patch of 16x16 pixels but should be a bigger \"window\" so to give the model a context for each patch. **The size of this window has to be refined**. A minor problem is to define the context for the patches in the border. This problem can be tackled by extending each image by mirroring its border so that also the border patches have a meaningful context. \n",
    "\n",
    "\n",
    "**TODO** try with more inputs, e.g. one input per 8x8 patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded a set of images\n",
    "n = 20\n",
    "\n",
    "imgs, gt_imgs = load_images(n)\n",
    "\n",
    "imgs[0].shape, gt_imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Define the window width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Extend images by mirroring\n",
    "First, as explained, we extend the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ext = extend_images(imgs, window_width)\n",
    "imgs_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just show an example\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "axs[0].imshow(imgs[1], cmap='Greys_r', vmin=0, vmax=1)\n",
    "\n",
    "mirror_width = int((window_width-16)/2)\n",
    "img_ext = imgs_ext[1]\n",
    "img_ext[mirror_width:-mirror_width, mirror_width-1:mirror_width+1] = np.array((0, 1, 0))\n",
    "img_ext[mirror_width:-mirror_width, -mirror_width-1:-mirror_width+1] = np.array((0, 1, 0))\n",
    "img_ext[mirror_width-1:mirror_width+1, mirror_width:-mirror_width] = np.array((0, 1, 0))\n",
    "img_ext[-mirror_width-1:-mirror_width+1, mirror_width:-mirror_width] = np.array((0, 1, 0))\n",
    "axs[1].imshow(img_ext, cmap='Greys_r', vmin=0, vmax=1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - For now avoid cross validation, just split the datasest in test and train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train, test = split_train_test(imgs_ext, gt_imgs, train_ratio=train_ratio, seed=1)\n",
    "train.imgs.shape, train.gt_imgs.shape, test.imgs.shape, test.gt_imgs.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Generate the inpus and the outputs\n",
    "Now we have to transform our images into **inputs** and **outputs** that will be fed to the CNN. From each image we extract **one input per patch** that had to be classified to **road (1)** or **background (0)** (400x400/16x16=625 inputs). Lastly, we will reshape each input to fulfill the requirements of the tensorflow library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.X, train.Y = imgs_to_inputs_outputs(train.imgs, train.gt_imgs, window_width)\n",
    "test.X, test.Y = imgs_to_inputs_outputs(test.imgs, test.gt_imgs, window_width)\n",
    "train.X.shape, train.Y.shape, test.X.shape, test.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the windows have the correct label\n",
    "nrow, ncol = 4, 4\n",
    "fig, axs = plt.subplots(nrow, ncol)\n",
    "fig.set_size_inches(5*nrow, 5*ncol)\n",
    "\n",
    "ind = int((window_width-16)/2)\n",
    "for k, window in enumerate(np.random.choice(range(train.X.shape[0]), size = nrow*ncol)):\n",
    "    i = int(k / ncol)\n",
    "    j = int(k % ncol)\n",
    "    \n",
    "    # just show the patch in the middle\n",
    "    wind = train.X[window]\n",
    "    \n",
    "    wind[ind:ind+16, ind-1:ind] = np.array([0, 1, 0])\n",
    "    wind[ind:ind+16, -ind:-ind+1] = np.array([0, 1, 0])\n",
    "    wind[ind-1:ind, ind:ind+16] = np.array([0, 1, 0])\n",
    "    wind[-ind:-ind+1, ind:ind+16] = np.array([0, 1, 0])\n",
    "\n",
    "    axs[i, j].imshow(wind, cmap='Greys_r', vmin=0, vmax=1)\n",
    "    axs[i, j].set_title(\"road\" if (train.Y[window][1]==1) else \"background\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap on **train** (and test) object:\n",
    "- **train.imgs**: (expanded) images of the training set. **shape**=(#train images, 456, 456, 3)\n",
    "- **train.gt_imgs**: respective groundtruth images. **shape**=(#train images, 400, 400)\n",
    "- **train.X**: inputs extracted from train.imgs. **shape**=(#total windows, 72, 72, 3)\n",
    "- **train.Y**: outputs extracted from train.gt_imgs. **shape**=(#total windows, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = 2\n",
    "model = Sequential()\n",
    "# 32 convolution filters\n",
    "# 3 rows in convolution kernel\n",
    "# 3 columns in convolution kernel\n",
    "# (1,28,28) is the shape of one input\n",
    "# strides=(1, 1) by default (step size?)\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=train.X[0].shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# MaxPooling2D is a way to reduce the number of parameters in our model by sliding \n",
    "# a 2x2 pooling filter across the previous layer and taking the max of the 4 values \n",
    "# in the 2x2 filter.\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25)) # a method for regularizing our model in order to prevent overfitting.\n",
    "\n",
    "# a convolutional neural network always ends with a fully connected layer followe by the ouput\n",
    "# layer\n",
    "# first flatten the weights of the convolution\n",
    "model.add(Flatten())\n",
    "# 128 = output size of the dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 2 = output size of the output dense layer (we have 2 classes)\n",
    "model.add(Dense(nclasses, activation='softmax'))\n",
    "\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fit the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train.X, train.Y, batch_size=32, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(test.X, test.Y, verbose=1)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_classes(test.X, verbose=1)\n",
    "stats(Z, test.Y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "display_ith_prediction(test, Z, i, window_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**problem**: it predicts always the same class [check here](https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
