{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flow:\n",
    "1. load, reshape and split in test and train all the images\n",
    "2. instantiate a cnn model \n",
    "3. run the model on train and test data to verify how many epochs are more or less needed to get a nice model\n",
    "4. use that number of epochs to run cross validation (pass the whole 'set_' of images to the cross validation). Repeat from 2 with another model and compare.\n",
    "\n",
    "\n",
    "- If you notice that you hardly overfit maybe remove/decrease the dropout layers (e.g. from 0.25 to 0.15)\n",
    "- Try building a model that predicts directly 16x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# keras functions\n",
    "from keras import callbacks\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# our libraries\n",
    "from preprocessing import *\n",
    "\n",
    "from cnn_models import *\n",
    "from evaluate import *\n",
    "\n",
    "import scipy as scipy\n",
    "from mask_to_submission import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The goal here is to use the CNN to reduce the size of the input image to obtain a \"discretized\" image of shape, e.g. (W/16, H/16). Every entry of this image is related to a patch in the input image. This obtained image is compared by the CNN with the groundtruth (after properly discretizing by it patch-wise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((400, 400, 3), (400, 400))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "n = 100\n",
    "\n",
    "imgs, gt_imgs = load_images(n)\n",
    "imgs[0].shape, gt_imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Reshape the data\n",
    "We reshape each input to fulfill our cnn inputs and output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 400, 400, 3), (100, 50, 50))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! set predict_patch_width in accordance to the model you are using !!!\n",
    "# the shape of the output of the model depends on the strides parameters \n",
    "# (if a layer has stride=2 then each ouput's side is half of the input'side).\n",
    "# predict_patch_width must be equal to the total reduction of the model, e.g.\n",
    "# if the model has three layer with stride=2 => the input of the model is \n",
    "# reduced by a factor of 2*2*2=8, i.e. the ouptut will be patch-wise with \n",
    "# patches 8x8 pixels.\n",
    "predict_patch_width = 8\n",
    "\n",
    "X, Y = images_to_XY(imgs, gt_imgs, predict_patch_width=predict_patch_width)\n",
    "\n",
    "set_ = SimpleNamespace()\n",
    "set_.X = X\n",
    "set_.Y = Y\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - For now avoid cross validation, just split the datasest in test and train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 400, 400, 3), (90, 50, 50), (10, 400, 400, 3), (10, 50, 50))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ratio = 0.2\n",
    "\n",
    "train, test = split_train_test(X, Y, test_ratio=test_ratio, seed=1)\n",
    "train.X.shape, train.Y.shape, test.X.shape, test.Y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # check it makes sense (show the i-th input of set_)\n",
    "# i = 0\n",
    "# set_ = test\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# axs[0].imshow(set_.Y[i, :, :, 1], cmap='gray')\n",
    "# axs[1].imshow(set_.X[i, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Build the CNN model or load a previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose one of the models you defined (with model_n) and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate an unique name for the model (so to avoid overwriting previous models)\n",
    "folder_name = \"model_\"+str('{0:%Y-%m-%d_%H%M%S}'.format(datetime.now()))\n",
    "model_path = \"..\\\\models\\\\\"+folder_name\n",
    "model = CnnModel(model_n=1, model_path=model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otherwise load a previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "model2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_101 (Conv2D)          (None, None, None, 32)    11648     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, None, None, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, None, None, 48)    38448     \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, None, None, 48)    57648     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, None, None, 48)    57648     \n",
      "_________________________________________________________________\n",
      "conv2d_105 (Conv2D)          (None, None, None, 48)    57648     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_106 (Conv2D)          (None, None, None, 64)    76864     \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, None, None, 64)    102464    \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, None, None, 2)     3202      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, None, None, 2)     0         \n",
      "=================================================================\n",
      "Total params: 405,570\n",
      "Trainable params: 405,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# give the folder\n",
    "folder_name = \"model_2017-12-16_185451\"\n",
    "model_path = \"..\\models\\\\\"+folder_name\n",
    "model = CnnModel(model_n = 1,model_path=model_path)\n",
    "model.load() # load the model and its weights\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Train the model on the train data while validating it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3058 - acc: 0.8651Epoch 00001: val_loss improved from inf to 0.31598, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 16s 682ms/step - loss: 0.3027 - acc: 0.8670 - val_loss: 0.3160 - val_acc: 0.8764\n",
      "Epoch 2/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3135 - acc: 0.8597Epoch 00002: val_loss improved from 0.31598 to 0.28613, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.3135 - acc: 0.8595 - val_loss: 0.2861 - val_acc: 0.8879\n",
      "Epoch 3/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2993 - acc: 0.8689Epoch 00003: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.3024 - acc: 0.8676 - val_loss: 0.3170 - val_acc: 0.8738\n",
      "Epoch 4/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2873 - acc: 0.8735Epoch 00004: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2873 - acc: 0.8734 - val_loss: 0.3253 - val_acc: 0.8779\n",
      "Epoch 5/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2844 - acc: 0.8741Epoch 00005: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.2889 - acc: 0.8721 - val_loss: 0.2926 - val_acc: 0.8832\n",
      "Epoch 6/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3108 - acc: 0.8645Epoch 00006: val_loss did not improve\n",
      "23/23 [==============================] - 12s 542ms/step - loss: 0.3073 - acc: 0.8663 - val_loss: 0.3596 - val_acc: 0.8732\n",
      "Epoch 7/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2883 - acc: 0.8752Epoch 00007: val_loss did not improve\n",
      "23/23 [==============================] - 12s 542ms/step - loss: 0.2904 - acc: 0.8744 - val_loss: 0.2934 - val_acc: 0.8778\n",
      "Epoch 8/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2925 - acc: 0.8738Epoch 00008: val_loss did not improve\n",
      "23/23 [==============================] - 13s 543ms/step - loss: 0.3014 - acc: 0.8688 - val_loss: 0.3085 - val_acc: 0.8776\n",
      "Epoch 9/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2970 - acc: 0.8711Epoch 00009: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2962 - acc: 0.8713 - val_loss: 0.3331 - val_acc: 0.8692\n",
      "Epoch 10/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3005 - acc: 0.8719Epoch 00010: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.3000 - acc: 0.8717 - val_loss: 0.3149 - val_acc: 0.8773\n",
      "Epoch 11/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3042 - acc: 0.8643Epoch 00011: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.3071 - acc: 0.8617 - val_loss: 0.3126 - val_acc: 0.8812\n",
      "Epoch 12/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2922 - acc: 0.8737Epoch 00012: val_loss improved from 0.28613 to 0.28221, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2965 - acc: 0.8706 - val_loss: 0.2822 - val_acc: 0.8954\n",
      "Epoch 13/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2949 - acc: 0.8719Epoch 00013: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2972 - acc: 0.8702 - val_loss: 0.2831 - val_acc: 0.8881\n",
      "Epoch 14/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2950 - acc: 0.8698Epoch 00014: val_loss improved from 0.28221 to 0.26028, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.2948 - acc: 0.8699 - val_loss: 0.2603 - val_acc: 0.8912\n",
      "Epoch 15/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2914 - acc: 0.8717Epoch 00015: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2906 - acc: 0.8721 - val_loss: 0.2738 - val_acc: 0.8925\n",
      "Epoch 16/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2862 - acc: 0.8781Epoch 00016: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2867 - acc: 0.8776 - val_loss: 0.3658 - val_acc: 0.8730\n",
      "Epoch 17/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2840 - acc: 0.8756Epoch 00017: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2854 - acc: 0.8751 - val_loss: 0.3134 - val_acc: 0.8785\n",
      "Epoch 18/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3104 - acc: 0.8629Epoch 00018: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.3047 - acc: 0.8662 - val_loss: 0.3289 - val_acc: 0.8810\n",
      "Epoch 19/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3144 - acc: 0.8616Epoch 00019: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.3123 - acc: 0.8627 - val_loss: 0.2730 - val_acc: 0.8885\n",
      "Epoch 20/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2876 - acc: 0.8740Epoch 00020: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2868 - acc: 0.8746 - val_loss: 0.2975 - val_acc: 0.8952\n",
      "Epoch 21/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2809 - acc: 0.8767Epoch 00021: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2830 - acc: 0.8750 - val_loss: 0.2956 - val_acc: 0.8862\n",
      "Epoch 22/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2971 - acc: 0.8720Epoch 00022: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2982 - acc: 0.8704 - val_loss: 0.2681 - val_acc: 0.8914\n",
      "Epoch 23/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2853 - acc: 0.8756Epoch 00023: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2883 - acc: 0.8745 - val_loss: 0.3022 - val_acc: 0.8810\n",
      "Epoch 24/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2829 - acc: 0.8773Epoch 00024: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2822 - acc: 0.8777 - val_loss: 0.3050 - val_acc: 0.8959\n",
      "Epoch 25/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2898 - acc: 0.8770Epoch 00025: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2896 - acc: 0.8771 - val_loss: 0.2642 - val_acc: 0.8964\n",
      "Epoch 26/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2820 - acc: 0.8757Epoch 00026: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2800 - acc: 0.8768 - val_loss: 0.3369 - val_acc: 0.8707\n",
      "Epoch 27/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3095 - acc: 0.8646Epoch 00027: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.3059 - acc: 0.8671 - val_loss: 0.3027 - val_acc: 0.8763\n",
      "Epoch 28/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2899 - acc: 0.8694Epoch 00028: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2924 - acc: 0.8680 - val_loss: 0.2943 - val_acc: 0.8891\n",
      "Epoch 29/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2784 - acc: 0.8805Epoch 00029: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2811 - acc: 0.8795 - val_loss: 0.2763 - val_acc: 0.8949\n",
      "Epoch 30/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2764 - acc: 0.8810Epoch 00030: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2804 - acc: 0.8792 - val_loss: 0.2718 - val_acc: 0.8996\n",
      "Epoch 31/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3066 - acc: 0.8629Epoch 00031: val_loss improved from 0.26028 to 0.25706, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.3033 - acc: 0.8650 - val_loss: 0.2571 - val_acc: 0.8962\n",
      "Epoch 32/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2954 - acc: 0.8719Epoch 00032: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2959 - acc: 0.8717 - val_loss: 0.2664 - val_acc: 0.8958\n",
      "Epoch 33/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2852 - acc: 0.8766Epoch 00033: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2853 - acc: 0.8767 - val_loss: 0.2688 - val_acc: 0.8986\n",
      "Epoch 34/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2752 - acc: 0.8787Epoch 00034: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2707 - acc: 0.8812 - val_loss: 0.2790 - val_acc: 0.9006\n",
      "Epoch 35/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2887 - acc: 0.8778Epoch 00035: val_loss improved from 0.25706 to 0.24993, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2866 - acc: 0.8789 - val_loss: 0.2499 - val_acc: 0.9013\n",
      "Epoch 36/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2753 - acc: 0.8794Epoch 00036: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2752 - acc: 0.8790 - val_loss: 0.2556 - val_acc: 0.9041\n",
      "Epoch 37/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2769 - acc: 0.8786Epoch 00037: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2786 - acc: 0.8785 - val_loss: 0.2952 - val_acc: 0.8904\n",
      "Epoch 38/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2824 - acc: 0.8776Epoch 00038: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2781 - acc: 0.8796 - val_loss: 0.2945 - val_acc: 0.8959\n",
      "Epoch 39/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2621 - acc: 0.8878Epoch 00039: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2628 - acc: 0.8876 - val_loss: 0.2970 - val_acc: 0.8986\n",
      "Epoch 40/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2838 - acc: 0.8754Epoch 00040: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2842 - acc: 0.8761 - val_loss: 0.3250 - val_acc: 0.8906\n",
      "Epoch 41/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2524 - acc: 0.8932Epoch 00041: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2517 - acc: 0.8935 - val_loss: 0.2982 - val_acc: 0.9004\n",
      "Epoch 42/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2847 - acc: 0.8749Epoch 00042: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2831 - acc: 0.8762 - val_loss: 0.2731 - val_acc: 0.8906\n",
      "Epoch 43/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2790 - acc: 0.8784Epoch 00043: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2793 - acc: 0.8779 - val_loss: 0.2904 - val_acc: 0.8990\n",
      "Epoch 44/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2612 - acc: 0.8844Epoch 00044: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2637 - acc: 0.8836 - val_loss: 0.2739 - val_acc: 0.9044\n",
      "Epoch 45/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2559 - acc: 0.8918Epoch 00045: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2604 - acc: 0.8887 - val_loss: 0.3077 - val_acc: 0.9009\n",
      "Epoch 46/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2696 - acc: 0.8824Epoch 00046: val_loss improved from 0.24993 to 0.24510, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2728 - acc: 0.8807 - val_loss: 0.2451 - val_acc: 0.9033\n",
      "Epoch 47/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2556 - acc: 0.8908Epoch 00047: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2533 - acc: 0.8916 - val_loss: 0.2707 - val_acc: 0.9048\n",
      "Epoch 48/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2886 - acc: 0.8787Epoch 00048: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2873 - acc: 0.8794 - val_loss: 0.2747 - val_acc: 0.9009\n",
      "Epoch 49/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2609 - acc: 0.8894Epoch 00049: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2587 - acc: 0.8904 - val_loss: 0.2915 - val_acc: 0.9023\n",
      "Epoch 50/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2696 - acc: 0.8848Epoch 00050: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2677 - acc: 0.8858 - val_loss: 0.2597 - val_acc: 0.9038\n",
      "Epoch 51/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2897 - acc: 0.8742Epoch 00051: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2870 - acc: 0.8757 - val_loss: 0.2649 - val_acc: 0.9032\n",
      "Epoch 52/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2622 - acc: 0.8898Epoch 00052: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2711 - acc: 0.8862 - val_loss: 0.2657 - val_acc: 0.9066\n",
      "Epoch 53/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2603 - acc: 0.8897Epoch 00053: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2589 - acc: 0.8902 - val_loss: 0.2738 - val_acc: 0.9087\n",
      "Epoch 54/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2564 - acc: 0.8893Epoch 00054: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2535 - acc: 0.8913 - val_loss: 0.2989 - val_acc: 0.8872\n",
      "Epoch 55/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2572 - acc: 0.8881Epoch 00055: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2609 - acc: 0.8860 - val_loss: 0.3073 - val_acc: 0.8946\n",
      "Epoch 56/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2470 - acc: 0.8928Epoch 00056: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2505 - acc: 0.8909 - val_loss: 0.2883 - val_acc: 0.9042\n",
      "Epoch 57/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3052 - acc: 0.8645Epoch 00057: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.3045 - acc: 0.8645 - val_loss: 0.3164 - val_acc: 0.8864\n",
      "Epoch 58/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2683 - acc: 0.8857Epoch 00058: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2715 - acc: 0.8837 - val_loss: 0.2630 - val_acc: 0.9090\n",
      "Epoch 59/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2908 - acc: 0.8750Epoch 00059: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2954 - acc: 0.8721 - val_loss: 0.2841 - val_acc: 0.8993\n",
      "Epoch 60/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2518 - acc: 0.8914Epoch 00060: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2527 - acc: 0.8910 - val_loss: 0.2947 - val_acc: 0.9016\n",
      "Epoch 61/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2543 - acc: 0.8925Epoch 00061: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2583 - acc: 0.8915 - val_loss: 0.2648 - val_acc: 0.9060\n",
      "Epoch 62/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2779 - acc: 0.8776Epoch 00062: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2784 - acc: 0.8777 - val_loss: 0.2745 - val_acc: 0.9000\n",
      "Epoch 63/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2786 - acc: 0.8817Epoch 00063: val_loss improved from 0.24510 to 0.24204, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2771 - acc: 0.8824 - val_loss: 0.2420 - val_acc: 0.9091\n",
      "Epoch 64/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2535 - acc: 0.8934Epoch 00064: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2563 - acc: 0.8920 - val_loss: 0.2541 - val_acc: 0.8985\n",
      "Epoch 65/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2268 - acc: 0.9049Epoch 00065: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2265 - acc: 0.9048 - val_loss: 0.2999 - val_acc: 0.8946\n",
      "Epoch 66/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2608 - acc: 0.8873Epoch 00066: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2617 - acc: 0.8870 - val_loss: 0.2634 - val_acc: 0.9079\n",
      "Epoch 67/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2506 - acc: 0.8927Epoch 00067: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2483 - acc: 0.8939 - val_loss: 0.3083 - val_acc: 0.8904\n",
      "Epoch 68/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2884 - acc: 0.8742Epoch 00068: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2892 - acc: 0.8735 - val_loss: 0.2557 - val_acc: 0.9016\n",
      "Epoch 69/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2774 - acc: 0.8828Epoch 00069: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2743 - acc: 0.8839 - val_loss: 0.2591 - val_acc: 0.9041\n",
      "Epoch 70/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2589 - acc: 0.8912Epoch 00070: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2604 - acc: 0.8907 - val_loss: 0.3115 - val_acc: 0.8824\n",
      "Epoch 71/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2997 - acc: 0.8691Epoch 00071: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2953 - acc: 0.8719 - val_loss: 0.2541 - val_acc: 0.9029\n",
      "Epoch 72/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2567 - acc: 0.8898Epoch 00072: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2556 - acc: 0.8900 - val_loss: 0.2976 - val_acc: 0.8952\n",
      "Epoch 73/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2541 - acc: 0.8910Epoch 00073: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2542 - acc: 0.8910 - val_loss: 0.2763 - val_acc: 0.9020\n",
      "Epoch 74/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2581 - acc: 0.8880Epoch 00074: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2553 - acc: 0.8892 - val_loss: 0.3399 - val_acc: 0.8903\n",
      "Epoch 75/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2646 - acc: 0.8863Epoch 00075: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2608 - acc: 0.8881 - val_loss: 0.3122 - val_acc: 0.8897\n",
      "Epoch 76/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2534 - acc: 0.8923Epoch 00076: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2521 - acc: 0.8932 - val_loss: 0.2605 - val_acc: 0.9026\n",
      "Epoch 77/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2493 - acc: 0.8958Epoch 00077: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2498 - acc: 0.8949 - val_loss: 0.2954 - val_acc: 0.8940\n",
      "Epoch 78/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2809 - acc: 0.8791Epoch 00078: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2788 - acc: 0.8798 - val_loss: 0.2845 - val_acc: 0.8953\n",
      "Epoch 79/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2676 - acc: 0.8866Epoch 00079: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2668 - acc: 0.8867 - val_loss: 0.2570 - val_acc: 0.9018\n",
      "Epoch 80/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2429 - acc: 0.8971Epoch 00080: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2417 - acc: 0.8975 - val_loss: 0.2426 - val_acc: 0.9120\n",
      "Epoch 81/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2470 - acc: 0.8957Epoch 00081: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2505 - acc: 0.8941 - val_loss: 0.2721 - val_acc: 0.9017\n",
      "Epoch 82/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2900 - acc: 0.8745Epoch 00082: val_loss improved from 0.24204 to 0.23590, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2901 - acc: 0.8743 - val_loss: 0.2359 - val_acc: 0.9081\n",
      "Epoch 83/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2431 - acc: 0.8974Epoch 00083: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2451 - acc: 0.8961 - val_loss: 0.2362 - val_acc: 0.9078\n",
      "Epoch 84/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2473 - acc: 0.8942Epoch 00084: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2559 - acc: 0.8908 - val_loss: 0.2851 - val_acc: 0.8932\n",
      "Epoch 85/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2671 - acc: 0.8853Epoch 00085: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2703 - acc: 0.8837 - val_loss: 0.2693 - val_acc: 0.8872\n",
      "Epoch 86/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2349 - acc: 0.9018Epoch 00086: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2336 - acc: 0.9022 - val_loss: 0.2680 - val_acc: 0.9004\n",
      "Epoch 87/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2451 - acc: 0.8977Epoch 00087: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2486 - acc: 0.8960 - val_loss: 0.2497 - val_acc: 0.9068\n",
      "Epoch 88/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2644 - acc: 0.8863Epoch 00088: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2638 - acc: 0.8866 - val_loss: 0.2679 - val_acc: 0.9020\n",
      "Epoch 89/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2344 - acc: 0.8991Epoch 00089: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2347 - acc: 0.8992 - val_loss: 0.2397 - val_acc: 0.9094\n",
      "Epoch 90/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2299 - acc: 0.9001Epoch 00090: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2337 - acc: 0.8988 - val_loss: 0.2425 - val_acc: 0.9120\n",
      "Epoch 91/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2345 - acc: 0.9012Epoch 00091: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2363 - acc: 0.9007 - val_loss: 0.2545 - val_acc: 0.9050\n",
      "Epoch 92/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2389 - acc: 0.8960Epoch 00092: val_loss improved from 0.23590 to 0.23444, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 552ms/step - loss: 0.2411 - acc: 0.8949 - val_loss: 0.2344 - val_acc: 0.9194\n",
      "Epoch 93/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2480 - acc: 0.8966Epoch 00093: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2513 - acc: 0.8951 - val_loss: 0.3091 - val_acc: 0.8982\n",
      "Epoch 94/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2771 - acc: 0.8788Epoch 00094: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2740 - acc: 0.8802 - val_loss: 0.2639 - val_acc: 0.9000\n",
      "Epoch 95/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2369 - acc: 0.9010Epoch 00095: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2359 - acc: 0.9014 - val_loss: 0.2621 - val_acc: 0.8986\n",
      "Epoch 96/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2581 - acc: 0.8901Epoch 00096: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2566 - acc: 0.8904 - val_loss: 0.2443 - val_acc: 0.9081\n",
      "Epoch 97/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2438 - acc: 0.8952Epoch 00097: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2414 - acc: 0.8966 - val_loss: 0.3050 - val_acc: 0.8962\n",
      "Epoch 98/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2232 - acc: 0.9066Epoch 00098: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2203 - acc: 0.9078 - val_loss: 0.2907 - val_acc: 0.9039\n",
      "Epoch 99/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2307 - acc: 0.9010Epoch 00099: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2269 - acc: 0.9027 - val_loss: 0.3157 - val_acc: 0.8922\n",
      "Epoch 100/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2478 - acc: 0.8952Epoch 00100: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2430 - acc: 0.8971 - val_loss: 0.2644 - val_acc: 0.9062\n",
      "Epoch 101/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2410 - acc: 0.8976Epoch 00101: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2404 - acc: 0.8987 - val_loss: 0.2695 - val_acc: 0.9152\n",
      "Epoch 102/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2838 - acc: 0.8780Epoch 00102: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2793 - acc: 0.8796 - val_loss: 0.2759 - val_acc: 0.9066\n",
      "Epoch 103/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2372 - acc: 0.9003Epoch 00103: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2358 - acc: 0.9010 - val_loss: 0.2870 - val_acc: 0.9064\n",
      "Epoch 104/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2373 - acc: 0.9005Epoch 00104: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2322 - acc: 0.9026 - val_loss: 0.2825 - val_acc: 0.9103\n",
      "Epoch 105/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2482 - acc: 0.8921Epoch 00105: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2491 - acc: 0.8914 - val_loss: 0.2564 - val_acc: 0.9065\n",
      "Epoch 106/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2571 - acc: 0.8923Epoch 00106: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2603 - acc: 0.8904 - val_loss: 0.2834 - val_acc: 0.9014\n",
      "Epoch 107/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2359 - acc: 0.8999Epoch 00107: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2345 - acc: 0.9009 - val_loss: 0.2758 - val_acc: 0.9066\n",
      "Epoch 108/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2185 - acc: 0.9099Epoch 00108: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2197 - acc: 0.9089 - val_loss: 0.3127 - val_acc: 0.8995\n",
      "Epoch 109/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2248 - acc: 0.9050Epoch 00109: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2234 - acc: 0.9053 - val_loss: 0.3614 - val_acc: 0.8814\n",
      "Epoch 110/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2512 - acc: 0.8928Epoch 00110: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2486 - acc: 0.8938 - val_loss: 0.2886 - val_acc: 0.9013\n",
      "Epoch 111/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2395 - acc: 0.8962Epoch 00111: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2355 - acc: 0.8979 - val_loss: 0.3161 - val_acc: 0.8995\n",
      "Epoch 112/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2248 - acc: 0.9057Epoch 00112: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2268 - acc: 0.9047 - val_loss: 0.2965 - val_acc: 0.9042\n",
      "Epoch 113/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2453 - acc: 0.8946Epoch 00113: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2475 - acc: 0.8931 - val_loss: 0.3036 - val_acc: 0.9001\n",
      "Epoch 114/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2639 - acc: 0.8893Epoch 00114: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2621 - acc: 0.8904 - val_loss: 0.2466 - val_acc: 0.9058\n",
      "Epoch 115/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2714 - acc: 0.8858Epoch 00115: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2706 - acc: 0.8872 - val_loss: 0.2384 - val_acc: 0.9058\n",
      "Epoch 116/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2435 - acc: 0.8964Epoch 00116: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2416 - acc: 0.8971 - val_loss: 0.2877 - val_acc: 0.8976\n",
      "Epoch 117/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2383 - acc: 0.9006Epoch 00117: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2397 - acc: 0.9003 - val_loss: 0.2824 - val_acc: 0.9075\n",
      "Epoch 118/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2288 - acc: 0.9034Epoch 00118: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2276 - acc: 0.9039 - val_loss: 0.2620 - val_acc: 0.9026\n",
      "Epoch 119/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2210 - acc: 0.9056Epoch 00119: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2214 - acc: 0.9054 - val_loss: 0.2766 - val_acc: 0.9020\n",
      "Epoch 120/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2370 - acc: 0.9000Epoch 00120: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2343 - acc: 0.9013 - val_loss: 0.2947 - val_acc: 0.9023\n",
      "Epoch 121/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2094 - acc: 0.9134Epoch 00121: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2114 - acc: 0.9124 - val_loss: 0.2606 - val_acc: 0.9121\n",
      "Epoch 122/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2205 - acc: 0.9060Epoch 00122: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2205 - acc: 0.9059 - val_loss: 0.2916 - val_acc: 0.9038\n",
      "Epoch 123/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2160 - acc: 0.9086Epoch 00123: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2148 - acc: 0.9095 - val_loss: 0.2868 - val_acc: 0.9046\n",
      "Epoch 124/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2313 - acc: 0.9017Epoch 00124: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2313 - acc: 0.9017 - val_loss: 0.2812 - val_acc: 0.9104\n",
      "Epoch 125/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2595 - acc: 0.8922Epoch 00125: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2602 - acc: 0.8918 - val_loss: 0.2401 - val_acc: 0.9011\n",
      "Epoch 126/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2584 - acc: 0.8945Epoch 00126: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2573 - acc: 0.8946 - val_loss: 0.2553 - val_acc: 0.9052\n",
      "Epoch 127/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2440 - acc: 0.8967Epoch 00127: val_loss improved from 0.23444 to 0.23322, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.2437 - acc: 0.8972 - val_loss: 0.2332 - val_acc: 0.9086\n",
      "Epoch 128/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2442 - acc: 0.8952Epoch 00128: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2433 - acc: 0.8958 - val_loss: 0.2498 - val_acc: 0.9093\n",
      "Epoch 129/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2368 - acc: 0.8998Epoch 00129: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2352 - acc: 0.9004 - val_loss: 0.2592 - val_acc: 0.9082\n",
      "Epoch 130/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2234 - acc: 0.9049Epoch 00130: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2214 - acc: 0.9061 - val_loss: 0.2576 - val_acc: 0.9076\n",
      "Epoch 131/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2514 - acc: 0.8950Epoch 00131: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2474 - acc: 0.8967 - val_loss: 0.3464 - val_acc: 0.8839\n",
      "Epoch 132/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2262 - acc: 0.9061Epoch 00132: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2249 - acc: 0.9068 - val_loss: 0.2591 - val_acc: 0.9079\n",
      "Epoch 133/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2373 - acc: 0.8998Epoch 00133: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2405 - acc: 0.8987 - val_loss: 0.2557 - val_acc: 0.9180\n",
      "Epoch 134/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2044 - acc: 0.9136Epoch 00134: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2042 - acc: 0.9136 - val_loss: 0.2805 - val_acc: 0.9106\n",
      "Epoch 135/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2363 - acc: 0.9003Epoch 00135: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2341 - acc: 0.9010 - val_loss: 0.2601 - val_acc: 0.9132\n",
      "Epoch 136/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2373 - acc: 0.8985Epoch 00136: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2381 - acc: 0.8976 - val_loss: 0.2797 - val_acc: 0.9158\n",
      "Epoch 137/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1987 - acc: 0.9169Epoch 00137: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2039 - acc: 0.9137 - val_loss: 0.2939 - val_acc: 0.9047\n",
      "Epoch 138/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2203 - acc: 0.9049Epoch 00138: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2187 - acc: 0.9057 - val_loss: 0.2430 - val_acc: 0.9160\n",
      "Epoch 139/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2172 - acc: 0.9110Epoch 00139: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2166 - acc: 0.9110 - val_loss: 0.2589 - val_acc: 0.9132\n",
      "Epoch 140/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2536 - acc: 0.8913Epoch 00140: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2509 - acc: 0.8928 - val_loss: 0.3040 - val_acc: 0.8760\n",
      "Epoch 141/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2688 - acc: 0.8855Epoch 00141: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2675 - acc: 0.8861 - val_loss: 0.3138 - val_acc: 0.8868\n",
      "Epoch 142/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2317 - acc: 0.9016Epoch 00142: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2338 - acc: 0.9008 - val_loss: 0.2653 - val_acc: 0.9132\n",
      "Epoch 143/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2058 - acc: 0.9115Epoch 00143: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2088 - acc: 0.9103 - val_loss: 0.3030 - val_acc: 0.9066\n",
      "Epoch 144/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2440 - acc: 0.8964Epoch 00144: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2433 - acc: 0.8963 - val_loss: 0.2427 - val_acc: 0.9202\n",
      "Epoch 145/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2679 - acc: 0.8848Epoch 00145: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2665 - acc: 0.8857 - val_loss: 0.2740 - val_acc: 0.8977\n",
      "Epoch 146/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2336 - acc: 0.8996Epoch 00146: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2318 - acc: 0.9004 - val_loss: 0.2893 - val_acc: 0.9010\n",
      "Epoch 147/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2186 - acc: 0.9094Epoch 00147: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2160 - acc: 0.9105 - val_loss: 0.3207 - val_acc: 0.8993\n",
      "Epoch 148/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2250 - acc: 0.9029Epoch 00148: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2220 - acc: 0.9042 - val_loss: 0.3559 - val_acc: 0.8922\n",
      "Epoch 149/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3530 - acc: 0.8478Epoch 00149: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.3537 - acc: 0.8472 - val_loss: 0.2852 - val_acc: 0.8758\n",
      "Epoch 150/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3276 - acc: 0.8603Epoch 00150: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.3257 - acc: 0.8611 - val_loss: 0.2578 - val_acc: 0.8883\n",
      "Epoch 151/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2779 - acc: 0.8812Epoch 00151: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2788 - acc: 0.8808 - val_loss: 0.2347 - val_acc: 0.9066\n",
      "Epoch 152/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2594 - acc: 0.8907Epoch 00152: val_loss improved from 0.23322 to 0.21404, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2562 - acc: 0.8925 - val_loss: 0.2140 - val_acc: 0.9129\n",
      "Epoch 153/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2391 - acc: 0.9022Epoch 00153: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2394 - acc: 0.9021 - val_loss: 0.2656 - val_acc: 0.8992\n",
      "Epoch 154/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2703 - acc: 0.8825Epoch 00154: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2724 - acc: 0.8817 - val_loss: 0.2205 - val_acc: 0.9159\n",
      "Epoch 155/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2868 - acc: 0.8745Epoch 00155: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2858 - acc: 0.8749 - val_loss: 0.2326 - val_acc: 0.9076\n",
      "Epoch 156/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2277 - acc: 0.9025Epoch 00156: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2296 - acc: 0.9021 - val_loss: 0.2182 - val_acc: 0.9162\n",
      "Epoch 157/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2109 - acc: 0.9126Epoch 00157: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2135 - acc: 0.9115 - val_loss: 0.2443 - val_acc: 0.9104\n",
      "Epoch 158/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2257 - acc: 0.9065Epoch 00158: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2324 - acc: 0.9038 - val_loss: 0.2415 - val_acc: 0.9078\n",
      "Epoch 159/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2377 - acc: 0.8992Epoch 00159: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2404 - acc: 0.8974 - val_loss: 0.2241 - val_acc: 0.9104\n",
      "Epoch 160/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2327 - acc: 0.9005Epoch 00160: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2296 - acc: 0.9022 - val_loss: 0.2305 - val_acc: 0.9177\n",
      "Epoch 161/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2511 - acc: 0.8956Epoch 00161: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2500 - acc: 0.8959 - val_loss: 0.2397 - val_acc: 0.9143\n",
      "Epoch 162/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2070 - acc: 0.9139Epoch 00162: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2065 - acc: 0.9141 - val_loss: 0.2515 - val_acc: 0.9107\n",
      "Epoch 163/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2368 - acc: 0.8972Epoch 00163: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2362 - acc: 0.8978 - val_loss: 0.2573 - val_acc: 0.9008\n",
      "Epoch 164/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9079Epoch 00164: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2201 - acc: 0.9081 - val_loss: 0.2643 - val_acc: 0.9083\n",
      "Epoch 165/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2575 - acc: 0.8901Epoch 00165: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2607 - acc: 0.8880 - val_loss: 0.2637 - val_acc: 0.8954\n",
      "Epoch 166/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2342 - acc: 0.9018Epoch 00166: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2368 - acc: 0.9006 - val_loss: 0.2636 - val_acc: 0.9061\n",
      "Epoch 167/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2297 - acc: 0.9037Epoch 00167: val_loss did not improve\n",
      "23/23 [==============================] - 13s 553ms/step - loss: 0.2315 - acc: 0.9023 - val_loss: 0.2241 - val_acc: 0.9156\n",
      "Epoch 168/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2493 - acc: 0.8938Epoch 00168: val_loss did not improve\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.2516 - acc: 0.8928 - val_loss: 0.2327 - val_acc: 0.9137\n",
      "Epoch 169/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2159 - acc: 0.9102Epoch 00169: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2131 - acc: 0.9112 - val_loss: 0.2463 - val_acc: 0.9132\n",
      "Epoch 170/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2440 - acc: 0.8977Epoch 00170: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2425 - acc: 0.8977 - val_loss: 0.2359 - val_acc: 0.9076\n",
      "Epoch 171/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2301 - acc: 0.9019Epoch 00171: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2291 - acc: 0.9023 - val_loss: 0.2261 - val_acc: 0.9104\n",
      "Epoch 172/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2246 - acc: 0.9033Epoch 00172: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2256 - acc: 0.9029 - val_loss: 0.2391 - val_acc: 0.9128\n",
      "Epoch 173/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2254 - acc: 0.9056Epoch 00173: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2283 - acc: 0.9035 - val_loss: 0.2325 - val_acc: 0.9135\n",
      "Epoch 174/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2205 - acc: 0.9057Epoch 00174: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2198 - acc: 0.9058 - val_loss: 0.2212 - val_acc: 0.9184\n",
      "Epoch 175/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2250 - acc: 0.9060Epoch 00175: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2238 - acc: 0.9064 - val_loss: 0.3229 - val_acc: 0.8965\n",
      "Epoch 176/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2125 - acc: 0.9108Epoch 00176: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2162 - acc: 0.9090 - val_loss: 0.2382 - val_acc: 0.9140\n",
      "Epoch 177/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2133 - acc: 0.9092Epoch 00177: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2101 - acc: 0.9105 - val_loss: 0.2696 - val_acc: 0.9150\n",
      "Epoch 178/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2234 - acc: 0.9045Epoch 00178: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2256 - acc: 0.9035 - val_loss: 0.2400 - val_acc: 0.9223\n",
      "Epoch 179/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2028 - acc: 0.9129Epoch 00179: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2019 - acc: 0.9134 - val_loss: 0.2793 - val_acc: 0.9069\n",
      "Epoch 180/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2147 - acc: 0.9088Epoch 00180: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2142 - acc: 0.9088 - val_loss: 0.2808 - val_acc: 0.9118\n",
      "Epoch 181/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2350 - acc: 0.9010Epoch 00181: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2340 - acc: 0.9018 - val_loss: 0.2568 - val_acc: 0.9118\n",
      "Epoch 182/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2112 - acc: 0.9103Epoch 00182: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2138 - acc: 0.9090 - val_loss: 0.2522 - val_acc: 0.9172\n",
      "Epoch 183/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2213 - acc: 0.9057Epoch 00183: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2204 - acc: 0.9060 - val_loss: 0.2932 - val_acc: 0.9018\n",
      "Epoch 184/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2157 - acc: 0.9082Epoch 00184: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2155 - acc: 0.9082 - val_loss: 0.2537 - val_acc: 0.9163\n",
      "Epoch 185/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2174 - acc: 0.9070Epoch 00185: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2150 - acc: 0.9084 - val_loss: 0.2690 - val_acc: 0.9123\n",
      "Epoch 186/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2163 - acc: 0.9076Epoch 00186: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2180 - acc: 0.9068 - val_loss: 0.2591 - val_acc: 0.9132\n",
      "Epoch 187/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2308 - acc: 0.9023Epoch 00187: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2335 - acc: 0.9006 - val_loss: 0.2501 - val_acc: 0.9147\n",
      "Epoch 188/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2229 - acc: 0.9066Epoch 00188: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2237 - acc: 0.9063 - val_loss: 0.2793 - val_acc: 0.9043\n",
      "Epoch 189/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2010 - acc: 0.9170Epoch 00189: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2052 - acc: 0.9148 - val_loss: 0.2739 - val_acc: 0.9113\n",
      "Epoch 190/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2105 - acc: 0.9104Epoch 00190: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2091 - acc: 0.9111 - val_loss: 0.2859 - val_acc: 0.9093\n",
      "Epoch 191/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2261 - acc: 0.9048Epoch 00191: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2253 - acc: 0.9053 - val_loss: 0.2717 - val_acc: 0.9123\n",
      "Epoch 192/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2122 - acc: 0.9105Epoch 00192: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2127 - acc: 0.9100 - val_loss: 0.2708 - val_acc: 0.9129\n",
      "Epoch 193/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2071 - acc: 0.9146Epoch 00193: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2030 - acc: 0.9164 - val_loss: 0.3219 - val_acc: 0.8948\n",
      "Epoch 194/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2050 - acc: 0.9143Epoch 00194: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2037 - acc: 0.9149 - val_loss: 0.2894 - val_acc: 0.9054\n",
      "Epoch 195/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2251 - acc: 0.9050Epoch 00195: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2280 - acc: 0.9034 - val_loss: 0.2840 - val_acc: 0.8968\n",
      "Epoch 196/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2156 - acc: 0.9092Epoch 00196: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2180 - acc: 0.9081 - val_loss: 0.2710 - val_acc: 0.9032\n",
      "Epoch 197/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2480 - acc: 0.8940Epoch 00197: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2473 - acc: 0.8944 - val_loss: 0.3248 - val_acc: 0.8906\n",
      "Epoch 198/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2204 - acc: 0.9098Epoch 00198: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2211 - acc: 0.9092 - val_loss: 0.2294 - val_acc: 0.9139\n",
      "Epoch 199/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2369 - acc: 0.8998Epoch 00199: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2359 - acc: 0.9001 - val_loss: 0.2468 - val_acc: 0.9026\n",
      "Epoch 200/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2161 - acc: 0.9062Epoch 00200: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2159 - acc: 0.9066 - val_loss: 0.2395 - val_acc: 0.9098\n",
      "Epoch 201/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2128 - acc: 0.9127Epoch 00201: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2098 - acc: 0.9138 - val_loss: 0.2460 - val_acc: 0.9137\n",
      "Epoch 202/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2118 - acc: 0.9091Epoch 00202: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2108 - acc: 0.9096 - val_loss: 0.3127 - val_acc: 0.8981\n",
      "Epoch 203/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2166 - acc: 0.9093Epoch 00203: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2192 - acc: 0.9081 - val_loss: 0.3493 - val_acc: 0.8884\n",
      "Epoch 204/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2365 - acc: 0.9018Epoch 00204: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2359 - acc: 0.9019 - val_loss: 0.2928 - val_acc: 0.9006\n",
      "Epoch 205/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2221 - acc: 0.9091Epoch 00205: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2232 - acc: 0.9081 - val_loss: 0.2908 - val_acc: 0.9018\n",
      "Epoch 206/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2241 - acc: 0.9046Epoch 00206: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2224 - acc: 0.9054 - val_loss: 0.5494 - val_acc: 0.8647\n",
      "Epoch 207/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2156 - acc: 0.9063Epoch 00207: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2153 - acc: 0.9066 - val_loss: 0.2587 - val_acc: 0.9187\n",
      "Epoch 208/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2191 - acc: 0.9094Epoch 00208: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2206 - acc: 0.9090 - val_loss: 0.2865 - val_acc: 0.9026\n",
      "Epoch 209/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1997 - acc: 0.9151Epoch 00209: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2036 - acc: 0.9137 - val_loss: 0.5532 - val_acc: 0.8728\n",
      "Epoch 210/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2105 - acc: 0.9112Epoch 00210: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2106 - acc: 0.9109 - val_loss: 0.2518 - val_acc: 0.9136\n",
      "Epoch 211/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2263 - acc: 0.9025Epoch 00211: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2311 - acc: 0.9004 - val_loss: 0.3369 - val_acc: 0.8966\n",
      "Epoch 212/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2142 - acc: 0.9084Epoch 00212: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2155 - acc: 0.9077 - val_loss: 0.3834 - val_acc: 0.8882\n",
      "Epoch 213/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1903 - acc: 0.9206Epoch 00213: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1937 - acc: 0.9198 - val_loss: 0.2966 - val_acc: 0.9082\n",
      "Epoch 214/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2407 - acc: 0.8977Epoch 00214: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2406 - acc: 0.8978 - val_loss: 0.3156 - val_acc: 0.8996\n",
      "Epoch 215/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2293 - acc: 0.9019Epoch 00215: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2281 - acc: 0.9031 - val_loss: 0.2540 - val_acc: 0.9099\n",
      "Epoch 216/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2239 - acc: 0.9081Epoch 00216: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2271 - acc: 0.9063 - val_loss: 0.3509 - val_acc: 0.8896\n",
      "Epoch 217/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2046 - acc: 0.9139Epoch 00217: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2066 - acc: 0.9128 - val_loss: 0.2906 - val_acc: 0.9094\n",
      "Epoch 218/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1984 - acc: 0.9181Epoch 00218: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2004 - acc: 0.9170 - val_loss: 0.3085 - val_acc: 0.9066\n",
      "Epoch 219/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2124 - acc: 0.9123Epoch 00219: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2152 - acc: 0.9107 - val_loss: 0.2645 - val_acc: 0.9066\n",
      "Epoch 220/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2216 - acc: 0.9071Epoch 00220: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2190 - acc: 0.9086 - val_loss: 0.2671 - val_acc: 0.9081\n",
      "Epoch 221/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1926 - acc: 0.9186Epoch 00221: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1908 - acc: 0.9197 - val_loss: 0.2947 - val_acc: 0.9036\n",
      "Epoch 222/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2111 - acc: 0.9108Epoch 00222: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2085 - acc: 0.9120 - val_loss: 0.2605 - val_acc: 0.9105\n",
      "Epoch 223/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2258 - acc: 0.9057Epoch 00223: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2242 - acc: 0.9063 - val_loss: 0.2776 - val_acc: 0.9102\n",
      "Epoch 224/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2168 - acc: 0.9072Epoch 00224: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2186 - acc: 0.9064 - val_loss: 0.3585 - val_acc: 0.8942\n",
      "Epoch 225/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1989 - acc: 0.9154Epoch 00225: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2076 - acc: 0.9107 - val_loss: 0.4135 - val_acc: 0.8758\n",
      "Epoch 226/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2306 - acc: 0.9029Epoch 00226: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2282 - acc: 0.9041 - val_loss: 0.3461 - val_acc: 0.8875\n",
      "Epoch 227/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2236 - acc: 0.9036Epoch 00227: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2240 - acc: 0.9035 - val_loss: 0.2841 - val_acc: 0.9085\n",
      "Epoch 228/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2157 - acc: 0.9104Epoch 00228: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2160 - acc: 0.9102 - val_loss: 0.2861 - val_acc: 0.9011\n",
      "Epoch 229/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2258 - acc: 0.9065Epoch 00229: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2239 - acc: 0.9074 - val_loss: 0.3105 - val_acc: 0.8938\n",
      "Epoch 230/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2298 - acc: 0.9047Epoch 00230: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2312 - acc: 0.9042 - val_loss: 0.2637 - val_acc: 0.9072\n",
      "Epoch 231/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2138 - acc: 0.9092Epoch 00231: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2125 - acc: 0.9097 - val_loss: 0.3066 - val_acc: 0.8952\n",
      "Epoch 232/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1945 - acc: 0.9185Epoch 00232: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1943 - acc: 0.9189 - val_loss: 0.2884 - val_acc: 0.9022\n",
      "Epoch 233/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2065 - acc: 0.9127Epoch 00233: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2091 - acc: 0.9117 - val_loss: 0.2626 - val_acc: 0.9038\n",
      "Epoch 234/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2973 - acc: 0.8722Epoch 00234: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2997 - acc: 0.8712 - val_loss: 0.4160 - val_acc: 0.8042\n",
      "Epoch 235/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3286 - acc: 0.8568Epoch 00235: val_loss improved from 0.21404 to 0.20912, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.3308 - acc: 0.8551 - val_loss: 0.2091 - val_acc: 0.9112\n",
      "Epoch 236/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2628 - acc: 0.8889Epoch 00236: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2600 - acc: 0.8904 - val_loss: 0.2334 - val_acc: 0.9135\n",
      "Epoch 237/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2226 - acc: 0.9055Epoch 00237: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2218 - acc: 0.9060 - val_loss: 0.2149 - val_acc: 0.9120\n",
      "Epoch 238/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2203 - acc: 0.9099Epoch 00238: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2187 - acc: 0.9105 - val_loss: 0.2350 - val_acc: 0.9069\n",
      "Epoch 239/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2143 - acc: 0.9115Epoch 00239: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2141 - acc: 0.9114 - val_loss: 0.2449 - val_acc: 0.9123\n",
      "Epoch 240/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2075 - acc: 0.9120Epoch 00240: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2078 - acc: 0.9125 - val_loss: 0.2336 - val_acc: 0.9158\n",
      "Epoch 241/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1979 - acc: 0.9179Epoch 00241: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1994 - acc: 0.9171 - val_loss: 0.2287 - val_acc: 0.9116\n",
      "Epoch 242/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2246 - acc: 0.9052Epoch 00242: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2243 - acc: 0.9049 - val_loss: 0.2475 - val_acc: 0.9104\n",
      "Epoch 243/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2085 - acc: 0.9139Epoch 00243: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2092 - acc: 0.9131 - val_loss: 0.2217 - val_acc: 0.9167\n",
      "Epoch 244/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2117 - acc: 0.9088Epoch 00244: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2105 - acc: 0.9095 - val_loss: 0.2527 - val_acc: 0.9166\n",
      "Epoch 245/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2016 - acc: 0.9149Epoch 00245: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1977 - acc: 0.9166 - val_loss: 0.2283 - val_acc: 0.9208\n",
      "Epoch 246/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1934 - acc: 0.9199Epoch 00246: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1957 - acc: 0.9191 - val_loss: 0.2303 - val_acc: 0.9164\n",
      "Epoch 247/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2153 - acc: 0.9085Epoch 00247: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2117 - acc: 0.9101 - val_loss: 0.2472 - val_acc: 0.9135\n",
      "Epoch 248/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2107 - acc: 0.9103Epoch 00248: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2102 - acc: 0.9109 - val_loss: 0.2452 - val_acc: 0.9144\n",
      "Epoch 249/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2112 - acc: 0.9112Epoch 00249: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2120 - acc: 0.9105 - val_loss: 0.2511 - val_acc: 0.9138\n",
      "Epoch 250/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2175 - acc: 0.9072Epoch 00250: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2167 - acc: 0.9078 - val_loss: 0.2433 - val_acc: 0.9129\n",
      "Epoch 251/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2170 - acc: 0.9077Epoch 00251: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2189 - acc: 0.9063 - val_loss: 0.3105 - val_acc: 0.8900\n",
      "Epoch 252/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2034 - acc: 0.9137Epoch 00252: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2051 - acc: 0.9129 - val_loss: 0.2482 - val_acc: 0.9140\n",
      "Epoch 253/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2100 - acc: 0.9134Epoch 00253: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2082 - acc: 0.9139 - val_loss: 0.2611 - val_acc: 0.9080\n",
      "Epoch 254/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2118 - acc: 0.9109Epoch 00254: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2147 - acc: 0.9096 - val_loss: 0.2440 - val_acc: 0.9150\n",
      "Epoch 255/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2049 - acc: 0.9129Epoch 00255: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2059 - acc: 0.9125 - val_loss: 0.2986 - val_acc: 0.9034\n",
      "Epoch 256/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1999 - acc: 0.9164Epoch 00256: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2018 - acc: 0.9154 - val_loss: 0.2827 - val_acc: 0.9067\n",
      "Epoch 257/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2253 - acc: 0.9069Epoch 00257: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2266 - acc: 0.9056 - val_loss: 0.2669 - val_acc: 0.9086\n",
      "Epoch 258/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2067 - acc: 0.9139Epoch 00258: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2117 - acc: 0.9112 - val_loss: 0.2659 - val_acc: 0.9087\n",
      "Epoch 259/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2028 - acc: 0.9125Epoch 00259: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2010 - acc: 0.9133 - val_loss: 0.3816 - val_acc: 0.8896\n",
      "Epoch 260/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2129 - acc: 0.9116Epoch 00260: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2112 - acc: 0.9129 - val_loss: 0.2452 - val_acc: 0.9132\n",
      "Epoch 261/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2142 - acc: 0.9095Epoch 00261: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2132 - acc: 0.9097 - val_loss: 0.3002 - val_acc: 0.9003\n",
      "Epoch 262/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2112 - acc: 0.9120Epoch 00262: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2117 - acc: 0.9117 - val_loss: 0.3031 - val_acc: 0.9011\n",
      "Epoch 263/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2079 - acc: 0.9110Epoch 00263: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2093 - acc: 0.9109 - val_loss: 0.3088 - val_acc: 0.8977\n",
      "Epoch 264/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1994 - acc: 0.9166Epoch 00264: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1997 - acc: 0.9164 - val_loss: 0.3044 - val_acc: 0.8951\n",
      "Epoch 265/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1951 - acc: 0.9149Epoch 00265: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1932 - acc: 0.9158 - val_loss: 0.3547 - val_acc: 0.8897\n",
      "Epoch 266/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2274 - acc: 0.9020Epoch 00266: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2302 - acc: 0.9006 - val_loss: 0.2635 - val_acc: 0.9027\n",
      "Epoch 267/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2231 - acc: 0.9038Epoch 00267: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2212 - acc: 0.9048 - val_loss: 0.2606 - val_acc: 0.9042\n",
      "Epoch 268/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2058 - acc: 0.9124Epoch 00268: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2040 - acc: 0.9133 - val_loss: 0.2622 - val_acc: 0.9080\n",
      "Epoch 269/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1939 - acc: 0.9195Epoch 00269: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1946 - acc: 0.9186 - val_loss: 0.4124 - val_acc: 0.8762\n",
      "Epoch 270/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1969 - acc: 0.9176Epoch 00270: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.1954 - acc: 0.9182 - val_loss: 0.2660 - val_acc: 0.9076\n",
      "Epoch 271/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2105 - acc: 0.9132Epoch 00271: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2126 - acc: 0.9118 - val_loss: 0.2674 - val_acc: 0.9049\n",
      "Epoch 272/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1949 - acc: 0.9182Epoch 00272: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1958 - acc: 0.9177 - val_loss: 0.3648 - val_acc: 0.8841\n",
      "Epoch 273/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2379 - acc: 0.8964Epoch 00273: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2357 - acc: 0.8979 - val_loss: 0.2773 - val_acc: 0.9036\n",
      "Epoch 274/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2004 - acc: 0.9140Epoch 00274: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2020 - acc: 0.9137 - val_loss: 0.3183 - val_acc: 0.9073\n",
      "Epoch 275/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2009 - acc: 0.9156Epoch 00275: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1992 - acc: 0.9161 - val_loss: 0.4881 - val_acc: 0.8800\n",
      "Epoch 276/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2075 - acc: 0.9119Epoch 00276: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2061 - acc: 0.9124 - val_loss: 0.3376 - val_acc: 0.9015\n",
      "Epoch 277/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1905 - acc: 0.9201Epoch 00277: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1915 - acc: 0.9198 - val_loss: 0.3761 - val_acc: 0.8972\n",
      "Epoch 278/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2124 - acc: 0.9098Epoch 00278: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2111 - acc: 0.9106 - val_loss: 0.2605 - val_acc: 0.9186\n",
      "Epoch 279/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2235 - acc: 0.9050Epoch 00279: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2219 - acc: 0.9057 - val_loss: 0.3800 - val_acc: 0.8844\n",
      "Epoch 280/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1933 - acc: 0.9195Epoch 00280: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1916 - acc: 0.9203 - val_loss: 0.3093 - val_acc: 0.9091\n",
      "Epoch 281/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2101 - acc: 0.9120Epoch 00281: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2065 - acc: 0.9135 - val_loss: 0.3186 - val_acc: 0.8902\n",
      "Epoch 282/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2188 - acc: 0.9062Epoch 00282: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2164 - acc: 0.9072 - val_loss: 0.2574 - val_acc: 0.9199\n",
      "Epoch 283/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2113 - acc: 0.9119Epoch 00283: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2090 - acc: 0.9132 - val_loss: 0.3503 - val_acc: 0.8906\n",
      "Epoch 284/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2152 - acc: 0.9088Epoch 00284: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2171 - acc: 0.9083 - val_loss: 0.2569 - val_acc: 0.9066\n",
      "Epoch 285/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2144 - acc: 0.9113Epoch 00285: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2147 - acc: 0.9111 - val_loss: 0.3123 - val_acc: 0.8972\n",
      "Epoch 286/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2102 - acc: 0.9127Epoch 00286: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2109 - acc: 0.9123 - val_loss: 0.2919 - val_acc: 0.9092\n",
      "Epoch 287/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1980 - acc: 0.9161Epoch 00287: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2026 - acc: 0.9138 - val_loss: 0.2753 - val_acc: 0.9100\n",
      "Epoch 288/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2079 - acc: 0.9127Epoch 00288: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2072 - acc: 0.9134 - val_loss: 0.2356 - val_acc: 0.9122\n",
      "Epoch 289/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2015 - acc: 0.9157Epoch 00289: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1978 - acc: 0.9171 - val_loss: 0.2668 - val_acc: 0.9091\n",
      "Epoch 290/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2158 - acc: 0.9086Epoch 00290: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2143 - acc: 0.9094 - val_loss: 0.3046 - val_acc: 0.8930\n",
      "Epoch 291/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2321 - acc: 0.9034Epoch 00291: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2332 - acc: 0.9030 - val_loss: 0.2559 - val_acc: 0.9088\n",
      "Epoch 292/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2454 - acc: 0.8974Epoch 00292: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2471 - acc: 0.8964 - val_loss: 0.2459 - val_acc: 0.9025\n",
      "Epoch 293/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2378 - acc: 0.8988Epoch 00293: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2349 - acc: 0.9000 - val_loss: 0.2572 - val_acc: 0.8934\n",
      "Epoch 294/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2170 - acc: 0.9103Epoch 00294: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2154 - acc: 0.9111 - val_loss: 0.4317 - val_acc: 0.8724\n",
      "Epoch 295/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1962 - acc: 0.9172Epoch 00295: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1971 - acc: 0.9169 - val_loss: 0.3319 - val_acc: 0.9007\n",
      "Epoch 296/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2274 - acc: 0.9028Epoch 00296: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2291 - acc: 0.9026 - val_loss: 0.3286 - val_acc: 0.8984\n",
      "Epoch 297/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2092 - acc: 0.9131Epoch 00297: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2076 - acc: 0.9138 - val_loss: 0.4350 - val_acc: 0.8780\n",
      "Epoch 298/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2103 - acc: 0.9109Epoch 00298: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2120 - acc: 0.9099 - val_loss: 0.3149 - val_acc: 0.9044\n",
      "Epoch 299/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2086 - acc: 0.9139Epoch 00299: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2074 - acc: 0.9143 - val_loss: 0.3132 - val_acc: 0.8968\n",
      "Epoch 300/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - acc: 0.9127Epoch 00300: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2053 - acc: 0.9131 - val_loss: 0.2739 - val_acc: 0.9057\n",
      "Epoch 301/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1976 - acc: 0.9161Epoch 00301: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1942 - acc: 0.9175 - val_loss: 0.3287 - val_acc: 0.8988\n",
      "Epoch 302/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1972 - acc: 0.9156Epoch 00302: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2005 - acc: 0.9141 - val_loss: 0.3014 - val_acc: 0.8991\n",
      "Epoch 303/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1935 - acc: 0.9181Epoch 00303: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1929 - acc: 0.9182 - val_loss: 0.2398 - val_acc: 0.9090\n",
      "Epoch 304/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1937 - acc: 0.9194Epoch 00304: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1961 - acc: 0.9185 - val_loss: 0.2723 - val_acc: 0.9012\n",
      "Epoch 305/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2243 - acc: 0.9026Epoch 00305: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2240 - acc: 0.9030 - val_loss: 0.2403 - val_acc: 0.9165\n",
      "Epoch 306/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2000 - acc: 0.9156Epoch 00306: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2018 - acc: 0.9154 - val_loss: 0.2685 - val_acc: 0.9106\n",
      "Epoch 307/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1965 - acc: 0.9174Epoch 00307: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1961 - acc: 0.9174 - val_loss: 0.2890 - val_acc: 0.9087\n",
      "Epoch 308/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1833 - acc: 0.9229Epoch 00308: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.1838 - acc: 0.9227 - val_loss: 0.5392 - val_acc: 0.8688\n",
      "Epoch 309/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9071Epoch 00309: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2240 - acc: 0.9056 - val_loss: 0.2507 - val_acc: 0.8992\n",
      "Epoch 310/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2315 - acc: 0.8998Epoch 00310: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2276 - acc: 0.9015 - val_loss: 0.5377 - val_acc: 0.8727\n",
      "Epoch 311/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2159 - acc: 0.9091Epoch 00311: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2171 - acc: 0.9087 - val_loss: 0.2702 - val_acc: 0.9125\n",
      "Epoch 312/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1925 - acc: 0.9179Epoch 00312: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1888 - acc: 0.9194 - val_loss: 0.4128 - val_acc: 0.8863\n",
      "Epoch 313/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - acc: 0.9156Epoch 00313: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2020 - acc: 0.9176 - val_loss: 0.2415 - val_acc: 0.9172\n",
      "Epoch 314/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.9232Epoch 00314: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1804 - acc: 0.9247 - val_loss: 0.3607 - val_acc: 0.8986\n",
      "Epoch 315/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2053 - acc: 0.9116Epoch 00315: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2057 - acc: 0.9117 - val_loss: 0.3571 - val_acc: 0.8966\n",
      "Epoch 316/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1887 - acc: 0.9193Epoch 00316: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1947 - acc: 0.9166 - val_loss: 0.2673 - val_acc: 0.9154\n",
      "Epoch 317/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1881 - acc: 0.9205Epoch 00317: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1879 - acc: 0.9207 - val_loss: 0.3352 - val_acc: 0.8988\n",
      "Epoch 318/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1926 - acc: 0.9178Epoch 00318: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1945 - acc: 0.9178 - val_loss: 0.4009 - val_acc: 0.8894\n",
      "Epoch 319/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1882 - acc: 0.9199Epoch 00319: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1902 - acc: 0.9189 - val_loss: 0.2608 - val_acc: 0.9181\n",
      "Epoch 320/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2073 - acc: 0.9115Epoch 00320: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2035 - acc: 0.9131 - val_loss: 0.3075 - val_acc: 0.8991\n",
      "Epoch 321/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2161 - acc: 0.9083Epoch 00321: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2158 - acc: 0.9084 - val_loss: 0.4414 - val_acc: 0.8806\n",
      "Epoch 322/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1813 - acc: 0.9251Epoch 00322: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1832 - acc: 0.9243 - val_loss: 0.4300 - val_acc: 0.8777\n",
      "Epoch 323/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1903 - acc: 0.9190Epoch 00323: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1913 - acc: 0.9191 - val_loss: 0.2565 - val_acc: 0.9102\n",
      "Epoch 324/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2068 - acc: 0.9120Epoch 00324: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2033 - acc: 0.9133 - val_loss: 0.2514 - val_acc: 0.9113\n",
      "Epoch 325/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2103 - acc: 0.9127Epoch 00325: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2100 - acc: 0.9129 - val_loss: 0.3252 - val_acc: 0.8948\n",
      "Epoch 326/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - acc: 0.9117Epoch 00326: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2031 - acc: 0.9128 - val_loss: 0.3124 - val_acc: 0.9026\n",
      "Epoch 327/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1877 - acc: 0.9232Epoch 00327: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1879 - acc: 0.9231 - val_loss: 0.2718 - val_acc: 0.9072\n",
      "Epoch 328/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2003 - acc: 0.9157Epoch 00328: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1983 - acc: 0.9168 - val_loss: 0.2510 - val_acc: 0.9112\n",
      "Epoch 329/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1928 - acc: 0.9165Epoch 00329: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1891 - acc: 0.9183 - val_loss: 0.2633 - val_acc: 0.9128\n",
      "Epoch 330/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1982 - acc: 0.9156Epoch 00330: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2012 - acc: 0.9145 - val_loss: 0.3073 - val_acc: 0.8978\n",
      "Epoch 331/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2094 - acc: 0.9119Epoch 00331: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2086 - acc: 0.9125 - val_loss: 0.2451 - val_acc: 0.9016\n",
      "Epoch 332/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2101 - acc: 0.9110Epoch 00332: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2085 - acc: 0.9120 - val_loss: 0.2482 - val_acc: 0.9046\n",
      "Epoch 333/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1924 - acc: 0.9185Epoch 00333: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1933 - acc: 0.9178 - val_loss: 0.2625 - val_acc: 0.9066\n",
      "Epoch 334/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2145 - acc: 0.9107Epoch 00334: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2144 - acc: 0.9101 - val_loss: 0.2701 - val_acc: 0.9074\n",
      "Epoch 335/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2064 - acc: 0.9115Epoch 00335: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2060 - acc: 0.9116 - val_loss: 0.2378 - val_acc: 0.9146\n",
      "Epoch 336/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2117 - acc: 0.9086Epoch 00336: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2163 - acc: 0.9067 - val_loss: 0.2879 - val_acc: 0.8995\n",
      "Epoch 337/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2172 - acc: 0.9073Epoch 00337: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2183 - acc: 0.9070 - val_loss: 0.5424 - val_acc: 0.8466\n",
      "Epoch 338/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2451 - acc: 0.8964Epoch 00338: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2459 - acc: 0.8959 - val_loss: 0.3301 - val_acc: 0.8862\n",
      "Epoch 339/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2118 - acc: 0.9114Epoch 00339: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2140 - acc: 0.9100 - val_loss: 0.3216 - val_acc: 0.8945\n",
      "Epoch 340/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2178 - acc: 0.9056Epoch 00340: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2187 - acc: 0.9052 - val_loss: 0.2763 - val_acc: 0.9081\n",
      "Epoch 341/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2110 - acc: 0.9124Epoch 00341: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2081 - acc: 0.9139 - val_loss: 0.2922 - val_acc: 0.9103\n",
      "Epoch 342/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1777 - acc: 0.9264Epoch 00342: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1785 - acc: 0.9261 - val_loss: 0.4236 - val_acc: 0.8819\n",
      "Epoch 343/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2040 - acc: 0.9136Epoch 00343: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2052 - acc: 0.9135 - val_loss: 0.3154 - val_acc: 0.8955\n",
      "Epoch 344/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2019 - acc: 0.9152Epoch 00344: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2009 - acc: 0.9160 - val_loss: 0.2688 - val_acc: 0.9126\n",
      "Epoch 345/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1847 - acc: 0.9233Epoch 00345: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1826 - acc: 0.9243 - val_loss: 0.3816 - val_acc: 0.8861\n",
      "Epoch 346/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2143 - acc: 0.9091Epoch 00346: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2147 - acc: 0.9090 - val_loss: 0.3209 - val_acc: 0.8895\n",
      "Epoch 347/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2130 - acc: 0.9091Epoch 00347: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2160 - acc: 0.9079 - val_loss: 0.2709 - val_acc: 0.9104\n",
      "Epoch 348/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1992 - acc: 0.9159Epoch 00348: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2054 - acc: 0.9128 - val_loss: 0.3823 - val_acc: 0.8908\n",
      "Epoch 349/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1896 - acc: 0.9211Epoch 00349: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1887 - acc: 0.9216 - val_loss: 0.4155 - val_acc: 0.8822\n",
      "Epoch 350/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2012 - acc: 0.9163Epoch 00350: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2088 - acc: 0.9138 - val_loss: 0.2642 - val_acc: 0.9093\n",
      "Epoch 351/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2002 - acc: 0.9152Epoch 00351: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2002 - acc: 0.9159 - val_loss: 0.3349 - val_acc: 0.8966\n",
      "Epoch 352/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2040 - acc: 0.9142Epoch 00352: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2034 - acc: 0.9146 - val_loss: 0.3495 - val_acc: 0.8972\n",
      "Epoch 353/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1870 - acc: 0.9220Epoch 00353: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1881 - acc: 0.9213 - val_loss: 0.3848 - val_acc: 0.8981\n",
      "Epoch 354/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2075 - acc: 0.9103Epoch 00354: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2080 - acc: 0.9101 - val_loss: 0.2616 - val_acc: 0.9132\n",
      "Epoch 355/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1809 - acc: 0.9236Epoch 00355: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1789 - acc: 0.9243 - val_loss: 0.3089 - val_acc: 0.9031\n",
      "Epoch 356/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1899 - acc: 0.9196Epoch 00356: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1891 - acc: 0.9201 - val_loss: 0.3037 - val_acc: 0.9023\n",
      "Epoch 357/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2012 - acc: 0.9161Epoch 00357: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1994 - acc: 0.9168 - val_loss: 0.4692 - val_acc: 0.8801\n",
      "Epoch 358/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1985 - acc: 0.9148Epoch 00358: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1988 - acc: 0.9149 - val_loss: 0.2815 - val_acc: 0.9081\n",
      "Epoch 359/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2008 - acc: 0.9155Epoch 00359: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2001 - acc: 0.9161 - val_loss: 0.2761 - val_acc: 0.9068\n",
      "Epoch 360/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1876 - acc: 0.9209Epoch 00360: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1899 - acc: 0.9196 - val_loss: 0.5842 - val_acc: 0.8707\n",
      "Epoch 361/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2127 - acc: 0.9098Epoch 00361: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2126 - acc: 0.9101 - val_loss: 0.2714 - val_acc: 0.9099\n",
      "Epoch 362/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2305 - acc: 0.9028Epoch 00362: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2311 - acc: 0.9024 - val_loss: 0.3879 - val_acc: 0.8847\n",
      "Epoch 363/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2101 - acc: 0.9096Epoch 00363: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2110 - acc: 0.9089 - val_loss: 0.3130 - val_acc: 0.8987\n",
      "Epoch 364/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1891 - acc: 0.9208Epoch 00364: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1930 - acc: 0.9185 - val_loss: 0.3078 - val_acc: 0.8994\n",
      "Epoch 365/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2022 - acc: 0.9133Epoch 00365: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2049 - acc: 0.9122 - val_loss: 0.3489 - val_acc: 0.9008\n",
      "Epoch 366/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1872 - acc: 0.9190Epoch 00366: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1892 - acc: 0.9182 - val_loss: 0.2765 - val_acc: 0.9090\n",
      "Epoch 367/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2008 - acc: 0.9131Epoch 00367: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1981 - acc: 0.9142 - val_loss: 0.2707 - val_acc: 0.9169\n",
      "Epoch 368/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1933 - acc: 0.9194Epoch 00368: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1933 - acc: 0.9196 - val_loss: 0.3353 - val_acc: 0.9075\n",
      "Epoch 369/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2027 - acc: 0.9145Epoch 00369: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2006 - acc: 0.9152 - val_loss: 0.2990 - val_acc: 0.9071\n",
      "Epoch 370/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1893 - acc: 0.9235Epoch 00370: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1890 - acc: 0.9237 - val_loss: 0.3919 - val_acc: 0.8932\n",
      "Epoch 371/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.9206Epoch 00371: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1846 - acc: 0.9201 - val_loss: 0.3035 - val_acc: 0.9063\n",
      "Epoch 372/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1856 - acc: 0.9213Epoch 00372: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1873 - acc: 0.9208 - val_loss: 0.2929 - val_acc: 0.9110\n",
      "Epoch 373/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1976 - acc: 0.9164Epoch 00373: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1991 - acc: 0.9156 - val_loss: 0.2618 - val_acc: 0.9161\n",
      "Epoch 374/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1940 - acc: 0.9187Epoch 00374: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1966 - acc: 0.9179 - val_loss: 0.2989 - val_acc: 0.9065\n",
      "Epoch 375/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2073 - acc: 0.9127Epoch 00375: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2050 - acc: 0.9139 - val_loss: 0.2979 - val_acc: 0.9045\n",
      "Epoch 376/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2220 - acc: 0.9050Epoch 00376: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2204 - acc: 0.9060 - val_loss: 0.4049 - val_acc: 0.8850\n",
      "Epoch 377/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2322 - acc: 0.9020Epoch 00377: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2305 - acc: 0.9029 - val_loss: 0.2986 - val_acc: 0.8923\n",
      "Epoch 378/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2223 - acc: 0.9077Epoch 00378: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2206 - acc: 0.9085 - val_loss: 0.2644 - val_acc: 0.9117\n",
      "Epoch 379/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1975 - acc: 0.9185Epoch 00379: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1975 - acc: 0.9179 - val_loss: 0.3545 - val_acc: 0.8896\n",
      "Epoch 380/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2022 - acc: 0.9156Epoch 00380: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2013 - acc: 0.9161 - val_loss: 0.2570 - val_acc: 0.9001\n",
      "Epoch 381/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2000 - acc: 0.9169Epoch 00381: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1994 - acc: 0.9168 - val_loss: 0.2756 - val_acc: 0.9075\n",
      "Epoch 382/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2008 - acc: 0.9162Epoch 00382: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1977 - acc: 0.9175 - val_loss: 0.3129 - val_acc: 0.9000\n",
      "Epoch 383/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1922 - acc: 0.9189Epoch 00383: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1942 - acc: 0.9178 - val_loss: 0.3131 - val_acc: 0.9036\n",
      "Epoch 384/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2096 - acc: 0.9123Epoch 00384: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2090 - acc: 0.9123 - val_loss: 0.2850 - val_acc: 0.9046\n",
      "Epoch 385/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1836 - acc: 0.9227Epoch 00385: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1817 - acc: 0.9233 - val_loss: 0.4080 - val_acc: 0.8925\n",
      "Epoch 386/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1933 - acc: 0.9177Epoch 00386: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1942 - acc: 0.9169 - val_loss: 0.3169 - val_acc: 0.9040\n",
      "Epoch 387/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1893 - acc: 0.9193Epoch 00387: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1902 - acc: 0.9188 - val_loss: 0.3050 - val_acc: 0.9066\n",
      "Epoch 388/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2000 - acc: 0.9158Epoch 00388: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1997 - acc: 0.9158 - val_loss: 0.3730 - val_acc: 0.8940\n",
      "Epoch 389/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1837 - acc: 0.9231Epoch 00389: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1836 - acc: 0.9234 - val_loss: 0.2849 - val_acc: 0.9109\n",
      "Epoch 390/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1879 - acc: 0.9188Epoch 00390: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1870 - acc: 0.9192 - val_loss: 0.3408 - val_acc: 0.9002\n",
      "Epoch 391/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1926 - acc: 0.9188Epoch 00391: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1910 - acc: 0.9197 - val_loss: 0.5432 - val_acc: 0.8779\n",
      "Epoch 392/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1989 - acc: 0.9140Epoch 00392: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2015 - acc: 0.9130 - val_loss: 0.4167 - val_acc: 0.8820\n",
      "Epoch 393/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1955 - acc: 0.9176Epoch 00393: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1927 - acc: 0.9189 - val_loss: 0.4034 - val_acc: 0.8890\n",
      "Epoch 394/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1834 - acc: 0.9225Epoch 00394: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1827 - acc: 0.9229 - val_loss: 0.4938 - val_acc: 0.8782\n",
      "Epoch 395/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1885 - acc: 0.9205Epoch 00395: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1879 - acc: 0.9208 - val_loss: 0.2990 - val_acc: 0.9027\n",
      "Epoch 396/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1962 - acc: 0.9168Epoch 00396: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1970 - acc: 0.9163 - val_loss: 0.3489 - val_acc: 0.8922\n",
      "Epoch 397/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1873 - acc: 0.9197Epoch 00397: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1881 - acc: 0.9191 - val_loss: 1.0281 - val_acc: 0.8288\n",
      "Epoch 398/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3730 - acc: 0.8394Epoch 00398: val_loss did not improve\n",
      "23/23 [==============================] - 13s 560ms/step - loss: 0.3710 - acc: 0.8401 - val_loss: 0.2625 - val_acc: 0.8800\n",
      "Epoch 399/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2574 - acc: 0.8890Epoch 00399: val_loss did not improve\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.2540 - acc: 0.8906 - val_loss: 0.3011 - val_acc: 0.8801\n",
      "Epoch 400/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2394 - acc: 0.9003Epoch 00400: val_loss did not improve\n",
      "23/23 [==============================] - 13s 555ms/step - loss: 0.2412 - acc: 0.8980 - val_loss: 0.2228 - val_acc: 0.9155\n",
      "Epoch 401/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2164 - acc: 0.9090Epoch 00401: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2168 - acc: 0.9089 - val_loss: 0.2625 - val_acc: 0.9018\n",
      "Epoch 402/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2010 - acc: 0.9164Epoch 00402: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2033 - acc: 0.9154 - val_loss: 0.2608 - val_acc: 0.9031\n",
      "Epoch 403/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2051 - acc: 0.9161Epoch 00403: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2073 - acc: 0.9149 - val_loss: 0.2710 - val_acc: 0.9041\n",
      "Epoch 404/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2075 - acc: 0.9124Epoch 00404: val_loss did not improve\n",
      "23/23 [==============================] - 13s 550ms/step - loss: 0.2048 - acc: 0.9136 - val_loss: 0.3977 - val_acc: 0.8838\n",
      "Epoch 405/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1926 - acc: 0.9207Epoch 00405: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1904 - acc: 0.9219 - val_loss: 0.2710 - val_acc: 0.9098\n",
      "Epoch 406/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2131 - acc: 0.9110Epoch 00406: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2100 - acc: 0.9123 - val_loss: 0.3500 - val_acc: 0.8962\n",
      "Epoch 407/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2067 - acc: 0.9121Epoch 00407: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2035 - acc: 0.9135 - val_loss: 0.2782 - val_acc: 0.9146\n",
      "Epoch 408/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1925 - acc: 0.9185Epoch 00408: val_loss did not improve\n",
      "23/23 [==============================] - 13s 555ms/step - loss: 0.1904 - acc: 0.9193 - val_loss: 0.3728 - val_acc: 0.8948\n",
      "Epoch 409/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1921 - acc: 0.9187Epoch 00409: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1898 - acc: 0.9199 - val_loss: 0.2928 - val_acc: 0.9077\n",
      "Epoch 410/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1921 - acc: 0.9172Epoch 00410: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1941 - acc: 0.9165 - val_loss: 0.2430 - val_acc: 0.9133\n",
      "Epoch 411/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2022 - acc: 0.9142Epoch 00411: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2054 - acc: 0.9132 - val_loss: 0.3079 - val_acc: 0.8998\n",
      "Epoch 412/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2058 - acc: 0.9120Epoch 00412: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2056 - acc: 0.9121 - val_loss: 0.2696 - val_acc: 0.9114\n",
      "Epoch 413/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1909 - acc: 0.9205Epoch 00413: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1955 - acc: 0.9185 - val_loss: 0.2810 - val_acc: 0.9130\n",
      "Epoch 414/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1861 - acc: 0.9213Epoch 00414: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1836 - acc: 0.9223 - val_loss: 0.3374 - val_acc: 0.8959\n",
      "Epoch 415/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1854 - acc: 0.9244Epoch 00415: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1837 - acc: 0.9251 - val_loss: 0.2929 - val_acc: 0.9118\n",
      "Epoch 416/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1963 - acc: 0.9168Epoch 00416: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1969 - acc: 0.9163 - val_loss: 0.3287 - val_acc: 0.9058\n",
      "Epoch 417/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1862 - acc: 0.9229Epoch 00417: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1835 - acc: 0.9241 - val_loss: 0.2838 - val_acc: 0.9086\n",
      "Epoch 418/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2012 - acc: 0.9156Epoch 00418: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1990 - acc: 0.9167 - val_loss: 0.2864 - val_acc: 0.9106\n",
      "Epoch 419/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1862 - acc: 0.9226Epoch 00419: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1879 - acc: 0.9221 - val_loss: 0.3811 - val_acc: 0.8924\n",
      "Epoch 420/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1912 - acc: 0.9201Epoch 00420: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1962 - acc: 0.9175 - val_loss: 0.3081 - val_acc: 0.9015\n",
      "Epoch 421/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1846 - acc: 0.9240Epoch 00421: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1856 - acc: 0.9233 - val_loss: 0.3065 - val_acc: 0.9025\n",
      "Epoch 422/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1828 - acc: 0.9220Epoch 00422: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1830 - acc: 0.9220 - val_loss: 0.3381 - val_acc: 0.8976\n",
      "Epoch 423/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1883 - acc: 0.9202Epoch 00423: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1887 - acc: 0.9198 - val_loss: 0.2897 - val_acc: 0.9094\n",
      "Epoch 424/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1963 - acc: 0.9147Epoch 00424: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1963 - acc: 0.9152 - val_loss: 0.3478 - val_acc: 0.9022\n",
      "Epoch 425/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1922 - acc: 0.9167Epoch 00425: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1921 - acc: 0.9170 - val_loss: 0.9237 - val_acc: 0.8202\n",
      "Epoch 426/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3132 - acc: 0.8695Epoch 00426: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.3065 - acc: 0.8723 - val_loss: 0.3865 - val_acc: 0.8647\n",
      "Epoch 427/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2311 - acc: 0.9012Epoch 00427: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2281 - acc: 0.9027 - val_loss: 0.2649 - val_acc: 0.9027\n",
      "Epoch 428/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2248 - acc: 0.9073Epoch 00428: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2244 - acc: 0.9072 - val_loss: 0.2425 - val_acc: 0.9129\n",
      "Epoch 429/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1972 - acc: 0.9186Epoch 00429: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1969 - acc: 0.9184 - val_loss: 0.3514 - val_acc: 0.8879\n",
      "Epoch 430/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2099 - acc: 0.9102Epoch 00430: val_loss improved from 0.20912 to 0.20689, saving model to ..\\models\\model_2017-12-16_185451\\2017-12-16_205955_best-weightsmodel2.hdf5\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.2098 - acc: 0.9103 - val_loss: 0.2069 - val_acc: 0.9163\n",
      "Epoch 431/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2093 - acc: 0.9141Epoch 00431: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2123 - acc: 0.9129 - val_loss: 0.2574 - val_acc: 0.9116\n",
      "Epoch 432/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2008 - acc: 0.9162Epoch 00432: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.2002 - acc: 0.9161 - val_loss: 0.3328 - val_acc: 0.8943\n",
      "Epoch 433/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1913 - acc: 0.9194Epoch 00433: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1886 - acc: 0.9207 - val_loss: 0.2671 - val_acc: 0.9156\n",
      "Epoch 434/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1980 - acc: 0.9152Epoch 00434: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1961 - acc: 0.9162 - val_loss: 0.2753 - val_acc: 0.9123\n",
      "Epoch 435/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2025 - acc: 0.9169Epoch 00435: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.2039 - acc: 0.9161 - val_loss: 0.2916 - val_acc: 0.9090\n",
      "Epoch 436/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1893 - acc: 0.9200Epoch 00436: val_loss did not improve\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.1939 - acc: 0.9173 - val_loss: 0.3852 - val_acc: 0.8892\n",
      "Epoch 437/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1889 - acc: 0.9210Epoch 00437: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1907 - acc: 0.9206 - val_loss: 0.3978 - val_acc: 0.8910\n",
      "Epoch 438/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1977 - acc: 0.9169Epoch 00438: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1965 - acc: 0.9174 - val_loss: 0.3319 - val_acc: 0.9041\n",
      "Epoch 439/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1965 - acc: 0.9163Epoch 00439: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1975 - acc: 0.9156 - val_loss: 0.2346 - val_acc: 0.9201\n",
      "Epoch 440/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2084 - acc: 0.9132Epoch 00440: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2081 - acc: 0.9131 - val_loss: 0.3253 - val_acc: 0.9008\n",
      "Epoch 441/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1822 - acc: 0.9226Epoch 00441: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1853 - acc: 0.9218 - val_loss: 0.2841 - val_acc: 0.9150\n",
      "Epoch 442/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1933 - acc: 0.9187Epoch 00442: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1908 - acc: 0.9199 - val_loss: 0.3771 - val_acc: 0.8882\n",
      "Epoch 443/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1948 - acc: 0.9148Epoch 00443: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1945 - acc: 0.9153 - val_loss: 0.2426 - val_acc: 0.9196\n",
      "Epoch 444/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2002 - acc: 0.9147Epoch 00444: val_loss did not improve\n",
      "23/23 [==============================] - 13s 548ms/step - loss: 0.1989 - acc: 0.9154 - val_loss: 0.3011 - val_acc: 0.9032\n",
      "Epoch 445/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1962 - acc: 0.9150Epoch 00445: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1951 - acc: 0.9155 - val_loss: 0.3246 - val_acc: 0.9028\n",
      "Epoch 446/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1793 - acc: 0.9227Epoch 00446: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1791 - acc: 0.9230 - val_loss: 0.3464 - val_acc: 0.8968\n",
      "Epoch 447/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1909 - acc: 0.9220Epoch 00447: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1883 - acc: 0.9229 - val_loss: 0.2908 - val_acc: 0.9019\n",
      "Epoch 448/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1871 - acc: 0.9210Epoch 00448: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1914 - acc: 0.9186 - val_loss: 0.4029 - val_acc: 0.8841\n",
      "Epoch 449/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1973 - acc: 0.9181Epoch 00449: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1979 - acc: 0.9179 - val_loss: 0.2487 - val_acc: 0.9188\n",
      "Epoch 450/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1957 - acc: 0.9184Epoch 00450: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1925 - acc: 0.9196 - val_loss: 0.3484 - val_acc: 0.8957\n",
      "Epoch 451/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1727 - acc: 0.9275Epoch 00451: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1713 - acc: 0.9281 - val_loss: 0.3192 - val_acc: 0.8987\n",
      "Epoch 452/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2085 - acc: 0.9093Epoch 00452: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2091 - acc: 0.9088 - val_loss: 0.4101 - val_acc: 0.8795\n",
      "Epoch 453/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1922 - acc: 0.9175Epoch 00453: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1942 - acc: 0.9167 - val_loss: 0.2522 - val_acc: 0.9166\n",
      "Epoch 454/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2228 - acc: 0.9032Epoch 00454: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2211 - acc: 0.9043 - val_loss: 0.3837 - val_acc: 0.8870\n",
      "Epoch 455/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1943 - acc: 0.9158Epoch 00455: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1936 - acc: 0.9159 - val_loss: 0.3236 - val_acc: 0.8970\n",
      "Epoch 456/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1714 - acc: 0.9286Epoch 00456: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1740 - acc: 0.9275 - val_loss: 0.2846 - val_acc: 0.9092\n",
      "Epoch 457/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1857 - acc: 0.9207Epoch 00457: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1878 - acc: 0.9201 - val_loss: 0.4778 - val_acc: 0.8797\n",
      "Epoch 458/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1969 - acc: 0.9149Epoch 00458: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1966 - acc: 0.9149 - val_loss: 0.2995 - val_acc: 0.9061\n",
      "Epoch 459/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2084 - acc: 0.9111Epoch 00459: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2088 - acc: 0.9107 - val_loss: 0.3044 - val_acc: 0.9035\n",
      "Epoch 460/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - acc: 0.9136Epoch 00460: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2045 - acc: 0.9142 - val_loss: 0.2607 - val_acc: 0.9137\n",
      "Epoch 461/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2114 - acc: 0.9095Epoch 00461: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2107 - acc: 0.9099 - val_loss: 0.2580 - val_acc: 0.9069\n",
      "Epoch 462/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1861 - acc: 0.9200Epoch 00462: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1832 - acc: 0.9215 - val_loss: 0.2430 - val_acc: 0.9195\n",
      "Epoch 463/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2150 - acc: 0.9106Epoch 00463: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2124 - acc: 0.9115 - val_loss: 0.3304 - val_acc: 0.8870\n",
      "Epoch 464/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1891 - acc: 0.9183Epoch 00464: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1923 - acc: 0.9169 - val_loss: 0.3102 - val_acc: 0.9019\n",
      "Epoch 465/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1803 - acc: 0.9255Epoch 00465: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1806 - acc: 0.9254 - val_loss: 0.3061 - val_acc: 0.9030\n",
      "Epoch 466/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2107 - acc: 0.9107Epoch 00466: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2103 - acc: 0.9112 - val_loss: 0.2690 - val_acc: 0.9122\n",
      "Epoch 467/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1740 - acc: 0.9300Epoch 00467: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1775 - acc: 0.9287 - val_loss: 0.2767 - val_acc: 0.9138\n",
      "Epoch 468/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1842 - acc: 0.9195Epoch 00468: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1887 - acc: 0.9180 - val_loss: 0.3512 - val_acc: 0.8973\n",
      "Epoch 469/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1852 - acc: 0.9217Epoch 00469: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1830 - acc: 0.9228 - val_loss: 0.3746 - val_acc: 0.8964\n",
      "Epoch 470/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1930 - acc: 0.9201Epoch 00470: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1953 - acc: 0.9189 - val_loss: 0.3925 - val_acc: 0.8882\n",
      "Epoch 471/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.9220Epoch 00471: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1852 - acc: 0.9218 - val_loss: 0.2865 - val_acc: 0.9111\n",
      "Epoch 472/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2003 - acc: 0.9150Epoch 00472: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1953 - acc: 0.9175 - val_loss: 0.2923 - val_acc: 0.9004\n",
      "Epoch 473/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1911 - acc: 0.9191Epoch 00473: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1897 - acc: 0.9196 - val_loss: 0.3214 - val_acc: 0.8946\n",
      "Epoch 474/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2000 - acc: 0.9157Epoch 00474: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1980 - acc: 0.9166 - val_loss: 0.2994 - val_acc: 0.9032\n",
      "Epoch 475/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1943 - acc: 0.9180Epoch 00475: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1963 - acc: 0.9171 - val_loss: 0.2486 - val_acc: 0.9121\n",
      "Epoch 476/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1814 - acc: 0.9228Epoch 00476: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1834 - acc: 0.9225 - val_loss: 0.2807 - val_acc: 0.9069\n",
      "Epoch 477/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1964 - acc: 0.9180Epoch 00477: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1943 - acc: 0.9190 - val_loss: 0.3007 - val_acc: 0.9060\n",
      "Epoch 478/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1735 - acc: 0.9290Epoch 00478: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1728 - acc: 0.9294 - val_loss: 0.2375 - val_acc: 0.9212\n",
      "Epoch 479/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1866 - acc: 0.9212Epoch 00479: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1840 - acc: 0.9222 - val_loss: 0.2787 - val_acc: 0.9107\n",
      "Epoch 480/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1985 - acc: 0.9156Epoch 00480: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2007 - acc: 0.9143 - val_loss: 0.3099 - val_acc: 0.9035\n",
      "Epoch 481/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1912 - acc: 0.9174Epoch 00481: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1929 - acc: 0.9172 - val_loss: 0.4007 - val_acc: 0.8870\n",
      "Epoch 482/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2122 - acc: 0.9080Epoch 00482: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2116 - acc: 0.9083 - val_loss: 0.2847 - val_acc: 0.9077\n",
      "Epoch 483/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1950 - acc: 0.9151Epoch 00483: val_loss did not improve\n",
      "23/23 [==============================] - 12s 542ms/step - loss: 0.1972 - acc: 0.9137 - val_loss: 0.3172 - val_acc: 0.9009\n",
      "Epoch 484/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1964 - acc: 0.9195Epoch 00484: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1974 - acc: 0.9189 - val_loss: 0.3177 - val_acc: 0.8996\n",
      "Epoch 485/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1782 - acc: 0.9243Epoch 00485: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1801 - acc: 0.9234 - val_loss: 0.2706 - val_acc: 0.9145\n",
      "Epoch 486/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1967 - acc: 0.9183Epoch 00486: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1940 - acc: 0.9193 - val_loss: 0.3664 - val_acc: 0.8950\n",
      "Epoch 487/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1841 - acc: 0.9226Epoch 00487: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1839 - acc: 0.9227 - val_loss: 0.2871 - val_acc: 0.9052\n",
      "Epoch 488/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1984 - acc: 0.9167Epoch 00488: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1962 - acc: 0.9176 - val_loss: 0.3433 - val_acc: 0.8973\n",
      "Epoch 489/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1987 - acc: 0.9171Epoch 00489: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1975 - acc: 0.9173 - val_loss: 0.2798 - val_acc: 0.9084\n",
      "Epoch 490/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1853 - acc: 0.9210Epoch 00490: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1841 - acc: 0.9219 - val_loss: 0.3402 - val_acc: 0.9045\n",
      "Epoch 491/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2015 - acc: 0.9149Epoch 00491: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2002 - acc: 0.9154 - val_loss: 0.2892 - val_acc: 0.9106\n",
      "Epoch 492/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1949 - acc: 0.9169Epoch 00492: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.2025 - acc: 0.9153 - val_loss: 0.2586 - val_acc: 0.9165\n",
      "Epoch 493/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9199Epoch 00493: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1815 - acc: 0.9209 - val_loss: 0.2919 - val_acc: 0.9059\n",
      "Epoch 494/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1773 - acc: 0.9247Epoch 00494: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1756 - acc: 0.9255 - val_loss: 0.3632 - val_acc: 0.8988\n",
      "Epoch 495/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1791 - acc: 0.9243Epoch 00495: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1771 - acc: 0.9255 - val_loss: 0.4277 - val_acc: 0.8840\n",
      "Epoch 496/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2066 - acc: 0.9143Epoch 00496: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2073 - acc: 0.9146 - val_loss: 0.3152 - val_acc: 0.9060\n",
      "Epoch 497/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1941 - acc: 0.9177Epoch 00497: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1957 - acc: 0.9170 - val_loss: 0.2624 - val_acc: 0.9177\n",
      "Epoch 498/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1849 - acc: 0.9217Epoch 00498: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1837 - acc: 0.9220 - val_loss: 0.3124 - val_acc: 0.9023\n",
      "Epoch 499/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1968 - acc: 0.9183Epoch 00499: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.2030 - acc: 0.9171 - val_loss: 0.3066 - val_acc: 0.9028\n",
      "Epoch 500/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1893 - acc: 0.9208Epoch 00500: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1881 - acc: 0.9214 - val_loss: 0.2917 - val_acc: 0.9106\n",
      "Epoch 501/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1842 - acc: 0.9230Epoch 00501: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1842 - acc: 0.9229 - val_loss: 0.3526 - val_acc: 0.8927\n",
      "Epoch 502/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1686 - acc: 0.9307Epoch 00502: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1677 - acc: 0.9308 - val_loss: 0.3204 - val_acc: 0.9039\n",
      "Epoch 503/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2076 - acc: 0.9112Epoch 00503: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2069 - acc: 0.9118 - val_loss: 0.3177 - val_acc: 0.9024\n",
      "Epoch 504/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1937 - acc: 0.9188Epoch 00504: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1927 - acc: 0.9193 - val_loss: 0.3301 - val_acc: 0.9031\n",
      "Epoch 505/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2002 - acc: 0.9158Epoch 00505: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1992 - acc: 0.9163 - val_loss: 0.3411 - val_acc: 0.9008\n",
      "Epoch 506/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1936 - acc: 0.9183Epoch 00506: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1960 - acc: 0.9173 - val_loss: 0.2959 - val_acc: 0.9068\n",
      "Epoch 507/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1888 - acc: 0.9196Epoch 00507: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1915 - acc: 0.9181 - val_loss: 0.3675 - val_acc: 0.8993\n",
      "Epoch 508/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1847 - acc: 0.9230Epoch 00508: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1830 - acc: 0.9235 - val_loss: 0.3937 - val_acc: 0.8910\n",
      "Epoch 509/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1911 - acc: 0.9175Epoch 00509: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1901 - acc: 0.9180 - val_loss: 0.3092 - val_acc: 0.9036\n",
      "Epoch 510/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1837 - acc: 0.9221Epoch 00510: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1878 - acc: 0.9212 - val_loss: 0.2648 - val_acc: 0.9154\n",
      "Epoch 511/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1875 - acc: 0.9213Epoch 00511: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1868 - acc: 0.9217 - val_loss: 0.4284 - val_acc: 0.8862\n",
      "Epoch 512/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1797 - acc: 0.9244Epoch 00512: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1812 - acc: 0.9235 - val_loss: 0.2860 - val_acc: 0.9163\n",
      "Epoch 513/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1719 - acc: 0.9292Epoch 00513: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1712 - acc: 0.9295 - val_loss: 0.3210 - val_acc: 0.9052\n",
      "Epoch 514/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9231Epoch 00514: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1840 - acc: 0.9228 - val_loss: 0.3258 - val_acc: 0.9005\n",
      "Epoch 515/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1908 - acc: 0.9197Epoch 00515: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1888 - acc: 0.9205 - val_loss: 0.3251 - val_acc: 0.9070\n",
      "Epoch 516/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2054 - acc: 0.9102Epoch 00516: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2040 - acc: 0.9108 - val_loss: 0.3123 - val_acc: 0.9042\n",
      "Epoch 517/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1850 - acc: 0.9231Epoch 00517: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1849 - acc: 0.9227 - val_loss: 0.3494 - val_acc: 0.9018\n",
      "Epoch 518/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2057 - acc: 0.9136Epoch 00518: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2037 - acc: 0.9146 - val_loss: 0.2167 - val_acc: 0.9168\n",
      "Epoch 519/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1913 - acc: 0.9210Epoch 00519: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1983 - acc: 0.9173 - val_loss: 0.2855 - val_acc: 0.8981\n",
      "Epoch 520/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1914 - acc: 0.9181Epoch 00520: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1909 - acc: 0.9182 - val_loss: 0.2928 - val_acc: 0.9044\n",
      "Epoch 521/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2001 - acc: 0.9136Epoch 00521: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1986 - acc: 0.9145 - val_loss: 0.3089 - val_acc: 0.8986\n",
      "Epoch 522/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1737 - acc: 0.9267Epoch 00522: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1778 - acc: 0.9247 - val_loss: 0.3453 - val_acc: 0.9055\n",
      "Epoch 523/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1795 - acc: 0.9244Epoch 00523: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1792 - acc: 0.9247 - val_loss: 0.3267 - val_acc: 0.8986\n",
      "Epoch 524/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1948 - acc: 0.9175Epoch 00524: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1971 - acc: 0.9171 - val_loss: 0.2285 - val_acc: 0.9248\n",
      "Epoch 525/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1930 - acc: 0.9190Epoch 00525: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1923 - acc: 0.9198 - val_loss: 0.2594 - val_acc: 0.9121\n",
      "Epoch 526/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1734 - acc: 0.9265Epoch 00526: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1725 - acc: 0.9268 - val_loss: 0.3334 - val_acc: 0.9021\n",
      "Epoch 527/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1920 - acc: 0.9177Epoch 00527: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1905 - acc: 0.9184 - val_loss: 0.2951 - val_acc: 0.9062\n",
      "Epoch 528/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1891 - acc: 0.9213Epoch 00528: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1871 - acc: 0.9223 - val_loss: 0.2520 - val_acc: 0.9187\n",
      "Epoch 529/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1825 - acc: 0.9247Epoch 00529: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1796 - acc: 0.9258 - val_loss: 0.3480 - val_acc: 0.8979\n",
      "Epoch 530/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1982 - acc: 0.9150Epoch 00530: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1997 - acc: 0.9141 - val_loss: 0.3798 - val_acc: 0.8976\n",
      "Epoch 531/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1826 - acc: 0.9223Epoch 00531: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1851 - acc: 0.9215 - val_loss: 0.3827 - val_acc: 0.9026\n",
      "Epoch 532/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2026 - acc: 0.9154Epoch 00532: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1998 - acc: 0.9167 - val_loss: 0.3120 - val_acc: 0.9030\n",
      "Epoch 533/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1817 - acc: 0.9257Epoch 00533: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1835 - acc: 0.9245 - val_loss: 0.2886 - val_acc: 0.9129\n",
      "Epoch 534/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1707 - acc: 0.9293Epoch 00534: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1697 - acc: 0.9297 - val_loss: 0.3291 - val_acc: 0.9026\n",
      "Epoch 535/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1838 - acc: 0.9204Epoch 00535: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1819 - acc: 0.9216 - val_loss: 0.4392 - val_acc: 0.8902\n",
      "Epoch 536/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1747 - acc: 0.9272Epoch 00536: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1743 - acc: 0.9272 - val_loss: 0.2770 - val_acc: 0.9160\n",
      "Epoch 537/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1911 - acc: 0.9183Epoch 00537: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1882 - acc: 0.9197 - val_loss: 0.2613 - val_acc: 0.9182\n",
      "Epoch 538/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1799 - acc: 0.9249Epoch 00538: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1793 - acc: 0.9254 - val_loss: 0.3612 - val_acc: 0.9019\n",
      "Epoch 539/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1841 - acc: 0.9226Epoch 00539: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1867 - acc: 0.9213 - val_loss: 0.2417 - val_acc: 0.9241\n",
      "Epoch 540/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1907 - acc: 0.9197Epoch 00540: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1874 - acc: 0.9211 - val_loss: 0.2969 - val_acc: 0.9088\n",
      "Epoch 541/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1974 - acc: 0.9160Epoch 00541: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1967 - acc: 0.9162 - val_loss: 0.4709 - val_acc: 0.8761\n",
      "Epoch 542/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1850 - acc: 0.9221Epoch 00542: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1856 - acc: 0.9216 - val_loss: 0.2521 - val_acc: 0.9164\n",
      "Epoch 543/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1944 - acc: 0.9170Epoch 00543: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1910 - acc: 0.9185 - val_loss: 0.3259 - val_acc: 0.9015\n",
      "Epoch 544/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1841 - acc: 0.9204Epoch 00544: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1884 - acc: 0.9187 - val_loss: 0.2559 - val_acc: 0.9146\n",
      "Epoch 545/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1868 - acc: 0.9196Epoch 00545: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1871 - acc: 0.9195 - val_loss: 0.2879 - val_acc: 0.9103\n",
      "Epoch 546/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1943 - acc: 0.9164Epoch 00546: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1919 - acc: 0.9176 - val_loss: 0.3010 - val_acc: 0.9038\n",
      "Epoch 547/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2012 - acc: 0.9119Epoch 00547: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1991 - acc: 0.9130 - val_loss: 0.2891 - val_acc: 0.9115\n",
      "Epoch 548/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1653 - acc: 0.9300Epoch 00548: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1699 - acc: 0.9285 - val_loss: 0.4081 - val_acc: 0.8911\n",
      "Epoch 549/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1968 - acc: 0.9168Epoch 00549: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1997 - acc: 0.9151 - val_loss: 0.2642 - val_acc: 0.9028\n",
      "Epoch 550/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2254 - acc: 0.9033Epoch 00550: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2303 - acc: 0.9004 - val_loss: 0.3890 - val_acc: 0.8785\n",
      "Epoch 551/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1670 - acc: 0.9304Epoch 00551: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1647 - acc: 0.9313 - val_loss: 0.3122 - val_acc: 0.9120\n",
      "Epoch 552/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1874 - acc: 0.9199Epoch 00552: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1848 - acc: 0.9213 - val_loss: 0.2822 - val_acc: 0.9058\n",
      "Epoch 553/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2004 - acc: 0.9152Epoch 00553: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1997 - acc: 0.9156 - val_loss: 0.3285 - val_acc: 0.8963\n",
      "Epoch 554/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1700 - acc: 0.9285Epoch 00554: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1685 - acc: 0.9294 - val_loss: 0.3218 - val_acc: 0.9091\n",
      "Epoch 555/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1881 - acc: 0.9193Epoch 00555: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1865 - acc: 0.9201 - val_loss: 0.2661 - val_acc: 0.9143\n",
      "Epoch 556/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1790 - acc: 0.9226Epoch 00556: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1790 - acc: 0.9228 - val_loss: 0.3519 - val_acc: 0.8983\n",
      "Epoch 557/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2052 - acc: 0.9147Epoch 00557: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2022 - acc: 0.9158 - val_loss: 0.3554 - val_acc: 0.8934\n",
      "Epoch 558/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1888 - acc: 0.9204Epoch 00558: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1892 - acc: 0.9201 - val_loss: 0.3400 - val_acc: 0.8995\n",
      "Epoch 559/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1908 - acc: 0.9186Epoch 00559: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1899 - acc: 0.9190 - val_loss: 0.3364 - val_acc: 0.9040\n",
      "Epoch 560/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1805 - acc: 0.9243Epoch 00560: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1823 - acc: 0.9234 - val_loss: 0.4471 - val_acc: 0.8834\n",
      "Epoch 561/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1970 - acc: 0.9174Epoch 00561: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1970 - acc: 0.9176 - val_loss: 0.2724 - val_acc: 0.9119\n",
      "Epoch 562/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1803 - acc: 0.9243Epoch 00562: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1809 - acc: 0.9233 - val_loss: 0.2716 - val_acc: 0.9128\n",
      "Epoch 563/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1634 - acc: 0.9315Epoch 00563: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1617 - acc: 0.9324 - val_loss: 0.3244 - val_acc: 0.9105\n",
      "Epoch 564/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1904 - acc: 0.9180Epoch 00564: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1909 - acc: 0.9179 - val_loss: 0.2966 - val_acc: 0.9083\n",
      "Epoch 565/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1826 - acc: 0.9222Epoch 00565: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1838 - acc: 0.9213 - val_loss: 0.2892 - val_acc: 0.9120\n",
      "Epoch 566/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1843 - acc: 0.9230Epoch 00566: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1837 - acc: 0.9227 - val_loss: 0.3961 - val_acc: 0.8942\n",
      "Epoch 567/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1963 - acc: 0.9154Epoch 00567: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1942 - acc: 0.9167 - val_loss: 0.5183 - val_acc: 0.8715\n",
      "Epoch 568/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1956 - acc: 0.9173Epoch 00568: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1922 - acc: 0.9187 - val_loss: 0.3816 - val_acc: 0.8934\n",
      "Epoch 569/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1834 - acc: 0.9224Epoch 00569: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1858 - acc: 0.9218 - val_loss: 0.2761 - val_acc: 0.9063\n",
      "Epoch 570/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1885 - acc: 0.9206Epoch 00570: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1889 - acc: 0.9203 - val_loss: 0.3191 - val_acc: 0.9066\n",
      "Epoch 571/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1864 - acc: 0.9211Epoch 00571: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1882 - acc: 0.9200 - val_loss: 0.2573 - val_acc: 0.9170\n",
      "Epoch 572/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1941 - acc: 0.9173Epoch 00572: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1931 - acc: 0.9176 - val_loss: 0.3612 - val_acc: 0.8940\n",
      "Epoch 573/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1847 - acc: 0.9219Epoch 00573: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1829 - acc: 0.9226 - val_loss: 0.2373 - val_acc: 0.9095\n",
      "Epoch 574/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2054 - acc: 0.9120Epoch 00574: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2058 - acc: 0.9116 - val_loss: 0.2727 - val_acc: 0.9146\n",
      "Epoch 575/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2141 - acc: 0.9093Epoch 00575: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2118 - acc: 0.9102 - val_loss: 0.2841 - val_acc: 0.9070\n",
      "Epoch 576/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2291 - acc: 0.9071Epoch 00576: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2278 - acc: 0.9074 - val_loss: 0.2956 - val_acc: 0.8849\n",
      "Epoch 577/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2049 - acc: 0.9119Epoch 00577: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.2056 - acc: 0.9115 - val_loss: 0.2817 - val_acc: 0.9102\n",
      "Epoch 578/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1998 - acc: 0.9137Epoch 00578: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1986 - acc: 0.9142 - val_loss: 0.2504 - val_acc: 0.9072\n",
      "Epoch 579/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1874 - acc: 0.9194Epoch 00579: val_loss did not improve\n",
      "23/23 [==============================] - 13s 549ms/step - loss: 0.1866 - acc: 0.9195 - val_loss: 0.2679 - val_acc: 0.9095\n",
      "Epoch 580/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1942 - acc: 0.9181Epoch 00580: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1945 - acc: 0.9182 - val_loss: 0.3825 - val_acc: 0.8847\n",
      "Epoch 581/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2027 - acc: 0.9149Epoch 00581: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2024 - acc: 0.9153 - val_loss: 0.3407 - val_acc: 0.9002\n",
      "Epoch 582/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9200Epoch 00582: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1832 - acc: 0.9206 - val_loss: 0.2798 - val_acc: 0.9080\n",
      "Epoch 583/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1825 - acc: 0.9230Epoch 00583: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1813 - acc: 0.9235 - val_loss: 0.3025 - val_acc: 0.9099\n",
      "Epoch 584/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1849 - acc: 0.9215Epoch 00584: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1850 - acc: 0.9214 - val_loss: 0.3855 - val_acc: 0.8862\n",
      "Epoch 585/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1808 - acc: 0.9231Epoch 00585: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1820 - acc: 0.9226 - val_loss: 0.2786 - val_acc: 0.9120\n",
      "Epoch 586/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1611 - acc: 0.9316Epoch 00586: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1611 - acc: 0.9313 - val_loss: 0.3083 - val_acc: 0.9023\n",
      "Epoch 587/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1967 - acc: 0.9165Epoch 00587: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1930 - acc: 0.9181 - val_loss: 0.3172 - val_acc: 0.8938\n",
      "Epoch 588/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1832 - acc: 0.9233Epoch 00588: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1861 - acc: 0.9221 - val_loss: 0.3312 - val_acc: 0.9047\n",
      "Epoch 589/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1842 - acc: 0.9201Epoch 00589: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1867 - acc: 0.9192 - val_loss: 0.3417 - val_acc: 0.8975\n",
      "Epoch 590/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1900 - acc: 0.9187Epoch 00590: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1879 - acc: 0.9195 - val_loss: 0.2933 - val_acc: 0.9078\n",
      "Epoch 591/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1832 - acc: 0.9206Epoch 00591: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1834 - acc: 0.9207 - val_loss: 0.2727 - val_acc: 0.9100\n",
      "Epoch 592/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1994 - acc: 0.9143Epoch 00592: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1982 - acc: 0.9149 - val_loss: 0.3033 - val_acc: 0.9046\n",
      "Epoch 593/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1862 - acc: 0.9209Epoch 00593: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1865 - acc: 0.9205 - val_loss: 0.4208 - val_acc: 0.8846\n",
      "Epoch 594/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2077 - acc: 0.9087Epoch 00594: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2127 - acc: 0.9061 - val_loss: 0.2936 - val_acc: 0.8864\n",
      "Epoch 595/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2090 - acc: 0.9112Epoch 00595: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2103 - acc: 0.9106 - val_loss: 0.3824 - val_acc: 0.8802\n",
      "Epoch 596/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1827 - acc: 0.9225Epoch 00596: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1788 - acc: 0.9242 - val_loss: 0.3078 - val_acc: 0.8990\n",
      "Epoch 597/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1819 - acc: 0.9234Epoch 00597: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1824 - acc: 0.9232 - val_loss: 0.3097 - val_acc: 0.9020\n",
      "Epoch 598/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1768 - acc: 0.9256Epoch 00598: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1771 - acc: 0.9254 - val_loss: 0.2903 - val_acc: 0.9101\n",
      "Epoch 599/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1883 - acc: 0.9207Epoch 00599: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1908 - acc: 0.9195 - val_loss: 0.3630 - val_acc: 0.8956\n",
      "Epoch 600/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1676 - acc: 0.9298Epoch 00600: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1674 - acc: 0.9300 - val_loss: 0.2734 - val_acc: 0.9164\n",
      "Epoch 601/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1830 - acc: 0.9222Epoch 00601: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1811 - acc: 0.9231 - val_loss: 0.3057 - val_acc: 0.9082\n",
      "Epoch 602/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1960 - acc: 0.9176Epoch 00602: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1957 - acc: 0.9179 - val_loss: 0.3674 - val_acc: 0.8924\n",
      "Epoch 603/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1868 - acc: 0.9192Epoch 00603: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1887 - acc: 0.9189 - val_loss: 0.3226 - val_acc: 0.9020\n",
      "Epoch 604/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1761 - acc: 0.9254Epoch 00604: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1817 - acc: 0.9232 - val_loss: 0.3166 - val_acc: 0.9042\n",
      "Epoch 605/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1834 - acc: 0.9232Epoch 00605: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1838 - acc: 0.9228 - val_loss: 0.3529 - val_acc: 0.8978\n",
      "Epoch 606/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1699 - acc: 0.9284Epoch 00606: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1682 - acc: 0.9292 - val_loss: 0.3257 - val_acc: 0.9088\n",
      "Epoch 607/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1995 - acc: 0.9173Epoch 00607: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1962 - acc: 0.9186 - val_loss: 0.3920 - val_acc: 0.8908\n",
      "Epoch 608/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1799 - acc: 0.9247Epoch 00608: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1847 - acc: 0.9220 - val_loss: 0.2621 - val_acc: 0.9135\n",
      "Epoch 609/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1914 - acc: 0.9186Epoch 00609: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1913 - acc: 0.9187 - val_loss: 0.2973 - val_acc: 0.9050\n",
      "Epoch 610/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1786 - acc: 0.9248Epoch 00610: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1836 - acc: 0.9223 - val_loss: 0.3461 - val_acc: 0.8932\n",
      "Epoch 611/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1821 - acc: 0.9241Epoch 00611: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1803 - acc: 0.9251 - val_loss: 0.2595 - val_acc: 0.9112\n",
      "Epoch 612/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2103 - acc: 0.9096Epoch 00612: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2095 - acc: 0.9099 - val_loss: 0.3390 - val_acc: 0.8937\n",
      "Epoch 613/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2124 - acc: 0.9121Epoch 00613: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.2100 - acc: 0.9129 - val_loss: 0.2716 - val_acc: 0.9097\n",
      "Epoch 614/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1803 - acc: 0.9243Epoch 00614: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1857 - acc: 0.9219 - val_loss: 0.2897 - val_acc: 0.9059\n",
      "Epoch 615/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2069 - acc: 0.9120Epoch 00615: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2033 - acc: 0.9137 - val_loss: 0.3266 - val_acc: 0.8925\n",
      "Epoch 616/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1799 - acc: 0.9240Epoch 00616: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1810 - acc: 0.9236 - val_loss: 0.2997 - val_acc: 0.9054\n",
      "Epoch 617/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1920 - acc: 0.9207Epoch 00617: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1951 - acc: 0.9190 - val_loss: 0.3148 - val_acc: 0.9012\n",
      "Epoch 618/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1682 - acc: 0.9286Epoch 00618: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1670 - acc: 0.9292 - val_loss: 0.3998 - val_acc: 0.8904\n",
      "Epoch 619/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1861 - acc: 0.9201Epoch 00619: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1877 - acc: 0.9193 - val_loss: 0.2873 - val_acc: 0.8984\n",
      "Epoch 620/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1901 - acc: 0.9188Epoch 00620: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1884 - acc: 0.9194 - val_loss: 0.2704 - val_acc: 0.9145\n",
      "Epoch 621/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2157 - acc: 0.9094Epoch 00621: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2114 - acc: 0.9113 - val_loss: 0.3063 - val_acc: 0.9015\n",
      "Epoch 622/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1884 - acc: 0.9195Epoch 00622: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1898 - acc: 0.9186 - val_loss: 0.3567 - val_acc: 0.8994\n",
      "Epoch 623/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1813 - acc: 0.9242Epoch 00623: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1793 - acc: 0.9252 - val_loss: 0.4077 - val_acc: 0.8890\n",
      "Epoch 624/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1929 - acc: 0.9196Epoch 00624: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1930 - acc: 0.9192 - val_loss: 0.2608 - val_acc: 0.9115\n",
      "Epoch 625/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1848 - acc: 0.9197Epoch 00625: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1848 - acc: 0.9195 - val_loss: 0.2617 - val_acc: 0.9144\n",
      "Epoch 626/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1986 - acc: 0.9169Epoch 00626: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1983 - acc: 0.9168 - val_loss: 0.3120 - val_acc: 0.8978\n",
      "Epoch 627/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.9207Epoch 00627: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1843 - acc: 0.9209 - val_loss: 0.3961 - val_acc: 0.9029\n",
      "Epoch 628/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1915 - acc: 0.9195Epoch 00628: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1960 - acc: 0.9173 - val_loss: 0.2778 - val_acc: 0.9078\n",
      "Epoch 629/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1921 - acc: 0.9178Epoch 00629: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1948 - acc: 0.9167 - val_loss: 0.2757 - val_acc: 0.9027\n",
      "Epoch 630/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1733 - acc: 0.9267Epoch 00630: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1715 - acc: 0.9277 - val_loss: 0.4080 - val_acc: 0.8926\n",
      "Epoch 631/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1797 - acc: 0.9225Epoch 00631: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1788 - acc: 0.9230 - val_loss: 0.2860 - val_acc: 0.9127\n",
      "Epoch 632/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1908 - acc: 0.9207Epoch 00632: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1910 - acc: 0.9207 - val_loss: 0.5221 - val_acc: 0.8658\n",
      "Epoch 633/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2032 - acc: 0.9157Epoch 00633: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2039 - acc: 0.9152 - val_loss: 0.2543 - val_acc: 0.9114\n",
      "Epoch 634/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1915 - acc: 0.9199Epoch 00634: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1910 - acc: 0.9201 - val_loss: 0.2931 - val_acc: 0.9094\n",
      "Epoch 635/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1719 - acc: 0.9267Epoch 00635: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1707 - acc: 0.9273 - val_loss: 0.2816 - val_acc: 0.9120\n",
      "Epoch 636/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1827 - acc: 0.9246Epoch 00636: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1825 - acc: 0.9248 - val_loss: 0.2973 - val_acc: 0.9099\n",
      "Epoch 637/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1921 - acc: 0.9170Epoch 00637: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1904 - acc: 0.9180 - val_loss: 0.3490 - val_acc: 0.9002\n",
      "Epoch 638/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1751 - acc: 0.9259Epoch 00638: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1789 - acc: 0.9245 - val_loss: 0.2906 - val_acc: 0.9110\n",
      "Epoch 639/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1810 - acc: 0.9229Epoch 00639: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1806 - acc: 0.9231 - val_loss: 0.3683 - val_acc: 0.8949\n",
      "Epoch 640/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1865 - acc: 0.9196Epoch 00640: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1858 - acc: 0.9201 - val_loss: 0.3898 - val_acc: 0.8941\n",
      "Epoch 641/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1777 - acc: 0.9230Epoch 00641: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1772 - acc: 0.9232 - val_loss: 0.3913 - val_acc: 0.8907\n",
      "Epoch 642/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1826 - acc: 0.9215Epoch 00642: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1799 - acc: 0.9227 - val_loss: 0.3138 - val_acc: 0.9071\n",
      "Epoch 643/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1690 - acc: 0.9278Epoch 00643: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1730 - acc: 0.9266 - val_loss: 0.3706 - val_acc: 0.8963\n",
      "Epoch 644/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1923 - acc: 0.9193Epoch 00644: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1927 - acc: 0.9189 - val_loss: 0.3003 - val_acc: 0.9058\n",
      "Epoch 645/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1872 - acc: 0.9214Epoch 00645: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1841 - acc: 0.9230 - val_loss: 0.3437 - val_acc: 0.8960\n",
      "Epoch 646/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1696 - acc: 0.9287Epoch 00646: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1681 - acc: 0.9295 - val_loss: 0.3367 - val_acc: 0.8998\n",
      "Epoch 647/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1806 - acc: 0.9248Epoch 00647: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1872 - acc: 0.9225 - val_loss: 0.3003 - val_acc: 0.9059\n",
      "Epoch 648/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1887 - acc: 0.9204Epoch 00648: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1877 - acc: 0.9206 - val_loss: 0.2800 - val_acc: 0.9100\n",
      "Epoch 649/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1931 - acc: 0.9206Epoch 00649: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1906 - acc: 0.9214 - val_loss: 0.3377 - val_acc: 0.8988\n",
      "Epoch 650/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1829 - acc: 0.9240Epoch 00650: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1862 - acc: 0.9219 - val_loss: 0.2904 - val_acc: 0.8997\n",
      "Epoch 651/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1819 - acc: 0.9220Epoch 00651: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1819 - acc: 0.9219 - val_loss: 0.2940 - val_acc: 0.9069\n",
      "Epoch 652/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2098 - acc: 0.9125Epoch 00652: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2064 - acc: 0.9139 - val_loss: 0.2398 - val_acc: 0.9036\n",
      "Epoch 653/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1829 - acc: 0.9221Epoch 00653: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1830 - acc: 0.9220 - val_loss: 0.2343 - val_acc: 0.9125\n",
      "Epoch 654/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2114 - acc: 0.9096Epoch 00654: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.2077 - acc: 0.9112 - val_loss: 0.2998 - val_acc: 0.8946\n",
      "Epoch 655/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1902 - acc: 0.9202Epoch 00655: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1920 - acc: 0.9192 - val_loss: 0.3467 - val_acc: 0.8980\n",
      "Epoch 656/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1858 - acc: 0.9210Epoch 00656: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1830 - acc: 0.9220 - val_loss: 0.2872 - val_acc: 0.9053\n",
      "Epoch 657/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1720 - acc: 0.9269Epoch 00657: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1730 - acc: 0.9262 - val_loss: 0.2834 - val_acc: 0.9127\n",
      "Epoch 658/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1879 - acc: 0.9190Epoch 00658: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1862 - acc: 0.9199 - val_loss: 0.2757 - val_acc: 0.9140\n",
      "Epoch 659/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1821 - acc: 0.9212Epoch 00659: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1805 - acc: 0.9220 - val_loss: 0.3051 - val_acc: 0.9084\n",
      "Epoch 660/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1982 - acc: 0.9178Epoch 00660: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2010 - acc: 0.9164 - val_loss: 0.2798 - val_acc: 0.9110\n",
      "Epoch 661/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2050 - acc: 0.9111Epoch 00661: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.2062 - acc: 0.9104 - val_loss: 0.2667 - val_acc: 0.9154\n",
      "Epoch 662/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1794 - acc: 0.9265Epoch 00662: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1778 - acc: 0.9272 - val_loss: 0.2999 - val_acc: 0.9055\n",
      "Epoch 663/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1783 - acc: 0.9245Epoch 00663: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1763 - acc: 0.9257 - val_loss: 0.2869 - val_acc: 0.9086\n",
      "Epoch 664/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1737 - acc: 0.9276Epoch 00664: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1743 - acc: 0.9273 - val_loss: 0.2570 - val_acc: 0.9168\n",
      "Epoch 665/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1916 - acc: 0.9169Epoch 00665: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1911 - acc: 0.9174 - val_loss: 0.2618 - val_acc: 0.9129\n",
      "Epoch 666/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1909 - acc: 0.9203Epoch 00666: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1893 - acc: 0.9210 - val_loss: 0.3401 - val_acc: 0.9002\n",
      "Epoch 667/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1804 - acc: 0.9245Epoch 00667: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1801 - acc: 0.9243 - val_loss: 0.2338 - val_acc: 0.9276\n",
      "Epoch 668/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1940 - acc: 0.9158Epoch 00668: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1903 - acc: 0.9175 - val_loss: 0.2869 - val_acc: 0.9101\n",
      "Epoch 669/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1649 - acc: 0.9320Epoch 00669: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1679 - acc: 0.9305 - val_loss: 0.2785 - val_acc: 0.9148\n",
      "Epoch 670/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1959 - acc: 0.9181Epoch 00670: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1948 - acc: 0.9183 - val_loss: 0.3481 - val_acc: 0.8987\n",
      "Epoch 671/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1854 - acc: 0.9220Epoch 00671: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1826 - acc: 0.9237 - val_loss: 0.2698 - val_acc: 0.9160\n",
      "Epoch 672/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1913 - acc: 0.9187Epoch 00672: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1884 - acc: 0.9203 - val_loss: 0.2640 - val_acc: 0.9174\n",
      "Epoch 673/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1822 - acc: 0.9232Epoch 00673: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1851 - acc: 0.9213 - val_loss: 0.3025 - val_acc: 0.9020\n",
      "Epoch 674/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1883 - acc: 0.9204Epoch 00674: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1871 - acc: 0.9211 - val_loss: 0.3801 - val_acc: 0.8974\n",
      "Epoch 675/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2152 - acc: 0.9122Epoch 00675: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2192 - acc: 0.9111 - val_loss: 0.3186 - val_acc: 0.8795\n",
      "Epoch 676/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3093 - acc: 0.8701Epoch 00676: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.3047 - acc: 0.8726 - val_loss: 0.2808 - val_acc: 0.8724\n",
      "Epoch 677/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2510 - acc: 0.8909Epoch 00677: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.2472 - acc: 0.8928 - val_loss: 0.2659 - val_acc: 0.8953\n",
      "Epoch 678/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2301 - acc: 0.9050Epoch 00678: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2281 - acc: 0.9059 - val_loss: 0.3004 - val_acc: 0.8818\n",
      "Epoch 679/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1773 - acc: 0.9256Epoch 00679: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1766 - acc: 0.9260 - val_loss: 0.2838 - val_acc: 0.9042\n",
      "Epoch 680/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1908 - acc: 0.9181Epoch 00680: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1896 - acc: 0.9188 - val_loss: 0.2620 - val_acc: 0.9028\n",
      "Epoch 681/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1684 - acc: 0.9281Epoch 00681: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1681 - acc: 0.9285 - val_loss: 0.3330 - val_acc: 0.8933\n",
      "Epoch 682/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1871 - acc: 0.9218Epoch 00682: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1888 - acc: 0.9206 - val_loss: 0.3061 - val_acc: 0.9070\n",
      "Epoch 683/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.9223Epoch 00683: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1850 - acc: 0.9217 - val_loss: 0.3758 - val_acc: 0.8947\n",
      "Epoch 684/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1835 - acc: 0.9219Epoch 00684: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1825 - acc: 0.9226 - val_loss: 0.3128 - val_acc: 0.9013\n",
      "Epoch 685/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1832 - acc: 0.9230Epoch 00685: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1835 - acc: 0.9230 - val_loss: 0.2758 - val_acc: 0.9090\n",
      "Epoch 686/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1916 - acc: 0.9166Epoch 00686: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1871 - acc: 0.9188 - val_loss: 0.2939 - val_acc: 0.9098\n",
      "Epoch 687/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1939 - acc: 0.9183Epoch 00687: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1954 - acc: 0.9175 - val_loss: 0.3051 - val_acc: 0.9020\n",
      "Epoch 688/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1808 - acc: 0.9246Epoch 00688: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1801 - acc: 0.9249 - val_loss: 0.3241 - val_acc: 0.9045\n",
      "Epoch 689/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1899 - acc: 0.9188Epoch 00689: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1896 - acc: 0.9190 - val_loss: 0.2866 - val_acc: 0.9062\n",
      "Epoch 690/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1911 - acc: 0.9189Epoch 00690: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1900 - acc: 0.9196 - val_loss: 0.4632 - val_acc: 0.8841\n",
      "Epoch 691/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1811 - acc: 0.9237Epoch 00691: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1815 - acc: 0.9236 - val_loss: 0.3174 - val_acc: 0.9058\n",
      "Epoch 692/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1876 - acc: 0.9191Epoch 00692: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1861 - acc: 0.9200 - val_loss: 0.3156 - val_acc: 0.9056\n",
      "Epoch 693/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1804 - acc: 0.9241Epoch 00693: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1773 - acc: 0.9255 - val_loss: 0.3007 - val_acc: 0.8997\n",
      "Epoch 694/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2120 - acc: 0.9085Epoch 00694: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.2089 - acc: 0.9102 - val_loss: 0.2878 - val_acc: 0.9097\n",
      "Epoch 695/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1833 - acc: 0.9235Epoch 00695: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1867 - acc: 0.9219 - val_loss: 0.2778 - val_acc: 0.9092\n",
      "Epoch 696/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1733 - acc: 0.9266Epoch 00696: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1752 - acc: 0.9257 - val_loss: 0.3042 - val_acc: 0.9136\n",
      "Epoch 697/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2007 - acc: 0.9135Epoch 00697: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1972 - acc: 0.9152 - val_loss: 0.2952 - val_acc: 0.9001\n",
      "Epoch 698/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1776 - acc: 0.9271Epoch 00698: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1769 - acc: 0.9273 - val_loss: 0.2938 - val_acc: 0.8941\n",
      "Epoch 699/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1702 - acc: 0.9280Epoch 00699: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1695 - acc: 0.9281 - val_loss: 0.2337 - val_acc: 0.9136\n",
      "Epoch 700/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1747 - acc: 0.9278Epoch 00700: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1734 - acc: 0.9281 - val_loss: 0.3243 - val_acc: 0.8974\n",
      "Epoch 701/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1813 - acc: 0.9235Epoch 00701: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1782 - acc: 0.9252 - val_loss: 0.3125 - val_acc: 0.8972\n",
      "Epoch 702/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1956 - acc: 0.9174Epoch 00702: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1925 - acc: 0.9189 - val_loss: 0.2898 - val_acc: 0.9036\n",
      "Epoch 703/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1774 - acc: 0.9253Epoch 00703: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1776 - acc: 0.9251 - val_loss: 0.2983 - val_acc: 0.9074\n",
      "Epoch 704/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1764 - acc: 0.9253Epoch 00704: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1763 - acc: 0.9253 - val_loss: 0.3006 - val_acc: 0.9051\n",
      "Epoch 705/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1620 - acc: 0.9308Epoch 00705: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1607 - acc: 0.9315 - val_loss: 0.4124 - val_acc: 0.8870\n",
      "Epoch 706/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1965 - acc: 0.9169Epoch 00706: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1946 - acc: 0.9175 - val_loss: 0.3432 - val_acc: 0.8965\n",
      "Epoch 707/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1677 - acc: 0.9300Epoch 00707: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1715 - acc: 0.9280 - val_loss: 0.3620 - val_acc: 0.8970\n",
      "Epoch 708/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1662 - acc: 0.9293Epoch 00708: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1685 - acc: 0.9283 - val_loss: 0.3216 - val_acc: 0.9054\n",
      "Epoch 709/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1704 - acc: 0.9284Epoch 00709: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1695 - acc: 0.9287 - val_loss: 0.3498 - val_acc: 0.8934\n",
      "Epoch 710/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1938 - acc: 0.9188Epoch 00710: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1960 - acc: 0.9181 - val_loss: 0.3089 - val_acc: 0.9004\n",
      "Epoch 711/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1697 - acc: 0.9294Epoch 00711: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1701 - acc: 0.9293 - val_loss: 0.2674 - val_acc: 0.9126\n",
      "Epoch 712/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1740 - acc: 0.9285Epoch 00712: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1754 - acc: 0.9278 - val_loss: 0.4066 - val_acc: 0.8872\n",
      "Epoch 713/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1782 - acc: 0.9252Epoch 00713: val_loss did not improve\n",
      "23/23 [==============================] - 13s 547ms/step - loss: 0.1804 - acc: 0.9241 - val_loss: 0.3532 - val_acc: 0.8953\n",
      "Epoch 714/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1797 - acc: 0.9242Epoch 00714: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1825 - acc: 0.9224 - val_loss: 0.2943 - val_acc: 0.8997\n",
      "Epoch 715/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1785 - acc: 0.9244Epoch 00715: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1788 - acc: 0.9243 - val_loss: 0.3302 - val_acc: 0.8947\n",
      "Epoch 716/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1864 - acc: 0.9210Epoch 00716: val_loss did not improve\n",
      "23/23 [==============================] - 13s 546ms/step - loss: 0.1864 - acc: 0.9211 - val_loss: 0.2912 - val_acc: 0.9092\n",
      "Epoch 717/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1778 - acc: 0.9229Epoch 00717: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1794 - acc: 0.9220 - val_loss: 0.3026 - val_acc: 0.9067\n",
      "Epoch 718/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1828 - acc: 0.9229Epoch 00718: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1834 - acc: 0.9224 - val_loss: 0.3339 - val_acc: 0.9010\n",
      "Epoch 719/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1838 - acc: 0.9228Epoch 00719: val_loss did not improve\n",
      "23/23 [==============================] - 12s 543ms/step - loss: 0.1837 - acc: 0.9226 - val_loss: 0.2879 - val_acc: 0.9155\n",
      "Epoch 720/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1724 - acc: 0.9265Epoch 00720: val_loss did not improve\n",
      "23/23 [==============================] - 13s 545ms/step - loss: 0.1726 - acc: 0.9263 - val_loss: 0.2701 - val_acc: 0.9128\n",
      "Epoch 721/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1828 - acc: 0.9209Epoch 00721: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1861 - acc: 0.9204 - val_loss: 0.3343 - val_acc: 0.9012\n",
      "Epoch 722/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1793 - acc: 0.9238Epoch 00722: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1776 - acc: 0.9246 - val_loss: 0.2941 - val_acc: 0.9092\n",
      "Epoch 723/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1720 - acc: 0.9259Epoch 00723: val_loss did not improve\n",
      "23/23 [==============================] - 13s 544ms/step - loss: 0.1705 - acc: 0.9268 - val_loss: 0.3683 - val_acc: 0.8932\n",
      "Epoch 724/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1838 - acc: 0.9225Epoch 00724: val_loss did not improve\n",
      "23/23 [==============================] - 13s 561ms/step - loss: 0.1854 - acc: 0.9220 - val_loss: 0.2879 - val_acc: 0.9089\n",
      "Epoch 725/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1970 - acc: 0.9165Epoch 00725: val_loss did not improve\n",
      "23/23 [==============================] - 13s 556ms/step - loss: 0.1960 - acc: 0.9167 - val_loss: 0.3040 - val_acc: 0.8990\n",
      "Epoch 726/900\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.1836 - acc: 0.9214Epoch 00726: val_loss did not improve\n",
      "23/23 [==============================] - 13s 551ms/step - loss: 0.1804 - acc: 0.9228 - val_loss: 0.3489 - val_acc: 0.9051\n",
      "Epoch 727/900\n",
      "18/23 [======================>.......] - ETA: 2s - loss: 0.1696 - acc: 0.9284Interrupt\n"
     ]
    }
   ],
   "source": [
    "# pass a batch size which is a factor of train.shape[0] so that all the batches are fo the same size\n",
    "num_epochs=900\n",
    "batch_size=4\n",
    "_ = model.train(train, test=test, num_epochs=num_epochs, batch_size=batch_size, monitor='val_loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAASOCAYAAABFWxHwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wepFhw6CIvsIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18203cb9da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Run cross validation to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "batch_size=3\n",
    "result = model.cross_validation(set_, batch_size=batch_size, num_epochs=num_epochs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the histories of the cross validation\n",
    "plot_history(result[\"history_mean\"]) \n",
    "# history of the folds (check if there is a worst case)\n",
    "# plot_history(result[\"histories\"][0]) \n",
    "# plot_history(result[\"histories\"][1]) \n",
    "# plot_history(result[\"histories\"][2]) \n",
    "# plot_history(result[\"histories\"][3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the accuracy and the loss obtained during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_epochs=20 # plot only the last n epochs\n",
    "model.plot_history(last_epochs=last_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the output of a specific layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these are all the layers \n",
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose a layer and an image \n",
    "image = test.X[0]\n",
    "layer_num = 8\n",
    "\n",
    "model.show_layer_output(image, layer_num, filename=\"\") # pass a filename if you want to store the image to file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#re-evaluate on some subset\n",
    "test_ratio = 0.10\n",
    "train, test = split_train_test(X, Y, test_ratio=test_ratio, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "0 (background)       0.83      0.99      0.90     18170\n",
      "      1 (road)       0.93      0.45      0.61      6830\n",
      "\n",
      "   avg / total       0.86      0.84      0.82     25000\n",
      "\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAGbCAYAAAAydGIFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182040064a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAGbCAYAAABqC/EcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f1a98780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the performance on train or test\n",
    "\n",
    "\n",
    "set_ = test\n",
    "\n",
    "model.evaluate_model(set_.X, set_.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and reset the train set as full dataset\n",
    "test_ratio = 0.0\n",
    "train, test = split_train_test(X, Y, test_ratio=test_ratio, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Show a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose an image to predict (or part of it)\n",
    "img = train.X[0][:, :]\n",
    "\n",
    "model.display_prediction(img, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cnn_models import *\n",
    "model.predict_and_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_name = \"model_\"+str('{0:%Y-%m-%d_%H%M%S}'.format(datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_leakyrelu_maxpooling\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    9472      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 128)   204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 128)   409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 64)    204864    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 2)     3202      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, None, None, 2)     0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, None, 2)     0         \n",
      "=================================================================\n",
      "Total params: 832,194\n",
      "Trainable params: 832,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[2,128,400,400]\n\t [[Node: training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_1/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](leaky_re_lu_2/sub, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_8)]]\n\nCaused by op 'training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"C:\\Program Files\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Program Files\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-2ff665f980a9>\", line 20, in <module>\n    _ = model.train(train, test=test, num_epochs=num_epochs, batch_size=batch_size, monitor='val_loss')\n  File \"C:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\", line 199, in train\n    callbacks=callbacks_list\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 1223, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1996, in fit_generator\n    self._make_train_function()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2389, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 555, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3082, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_1/MaxPool', defined at:\n  File \"C:\\Program Files\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-2ff665f980a9>\", line 16, in <module>\n    model = CnnModel(model_n=i, model_path=model_path)\n  File \"C:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\", line 49, in __init__\n    self.model = models[model_n]()\n  File \"C:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\", line 987, in model_leakyrelu_maxpooling\n    model.add(MaxPooling2D(padding=\"same\",pool_size=pool_size))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3460, in pool2d\n    data_format=tf_data_format)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1958, in max_pool\n    name=name)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2805, in _max_pool\n    data_format=data_format, name=name)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2,128,400,400]\n\t [[Node: training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_1/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](leaky_re_lu_2/sub, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_8)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2,128,400,400]\n\t [[Node: training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_1/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](leaky_re_lu_2/sub, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_8)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2ff665f980a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train, test, num_epochs, batch_size, monitor, save_history)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             )\n\u001b[1;32m    201\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msave_history\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2112\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2113\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2,128,400,400]\n\t [[Node: training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_1/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](leaky_re_lu_2/sub, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_8)]]\n\nCaused by op 'training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"C:\\Program Files\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Program Files\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-2ff665f980a9>\", line 20, in <module>\n    _ = model.train(train, test=test, num_epochs=num_epochs, batch_size=batch_size, monitor='val_loss')\n  File \"C:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\", line 199, in train\n    callbacks=callbacks_list\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 1223, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1996, in fit_generator\n    self._make_train_function()\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2389, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 555, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3082, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'max_pooling2d_1/MaxPool', defined at:\n  File \"C:\\Program Files\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-2ff665f980a9>\", line 16, in <module>\n    model = CnnModel(model_n=i, model_path=model_path)\n  File \"C:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\", line 49, in __init__\n    self.model = models[model_n]()\n  File \"C:\\Users\\Val\\Desktop\\road-segmentation\\scripts\\cnn_models.py\", line 987, in model_leakyrelu_maxpooling\n    model.add(MaxPooling2D(padding=\"same\",pool_size=pool_size))\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\", line 489, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3460, in pool2d\n    data_format=tf_data_format)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1958, in max_pool\n    name=name)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 2805, in _max_pool\n    data_format=data_format, name=name)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2,128,400,400]\n\t [[Node: training/Adam/gradients/max_pooling2d_1/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@max_pooling2d_1/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](leaky_re_lu_2/sub, max_pooling2d_1/MaxPool, training/Adam/gradients/AddN_8)]]\n"
     ]
    }
   ],
   "source": [
    "from cnn_models import *\n",
    "#models = [\n",
    "#0          model1,\n",
    "#1          model2,\n",
    "#2          additional_conv_layer_model,\n",
    "#3          max_pooling_model,\n",
    "#4          leaky_relu_model,\n",
    "#5          decreased_dropout_model,\n",
    "#6          many_filters_model, \n",
    "#7          model_leakyrelu_maxpooling, \n",
    "#8          model_relu_maxpooling] \n",
    "models_to_train =  [7]\n",
    "for i in models_to_train:\n",
    "    # generate an unique name for the model (so to avoid overwriting previous models)\n",
    "    model_path = \"..\\\\models\\\\\"+folder_name\n",
    "    model = CnnModel(model_n=i, model_path=model_path)\n",
    "    model.summary()\n",
    "    num_epochs=3\n",
    "    batch_size=2\n",
    "    _ = model.train(train, test=test, num_epochs=num_epochs, batch_size=batch_size, monitor='val_loss') \n",
    "    model.save()\n",
    "    model.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some callbacks example: \n",
    "\n",
    "# create a list of callbacks we want to use during training\n",
    "# # a callback to store epoch results to a csv file\n",
    "# filename='model_train_new.csv'\n",
    "# csv_log = callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "\n",
    "# # a callback to stob before doing the predefined number of epochs (stop before overfitting the data)\n",
    "# early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')\n",
    "\n",
    "# # a callback to save the best model (best model = the one with the lowest 'monitor' variable)\n",
    "# filepath = \"best-weights-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5\"\n",
    "# checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# # callbacks_list = [csv_log,early_stopping,checkpoint]\n",
    "\n",
    "print(\n",
    "    \"-get configurations:\", \"\\n\",\n",
    "    model.get_config(), \"\\n\",\n",
    "    model.layers[0].get_config(), \"\\n\",\n",
    "\n",
    "    \"\\n-get shapes\", \"\\n\",\n",
    "    model.layers[0].input_shape, \"\\n\",\n",
    "    model.layers[0].output_shape, \"\\n\",\n",
    "    \n",
    "    \"\\n-get weights\", \"\\n\",\n",
    "    model.layers[0].get_weights()[0].shape, \"\\n\",\n",
    "    \n",
    "    \"\\n-check if trainable\", \"\\n\",\n",
    "    model.layers[0].trainable, \"\\n\", # you can set this to false to \"freeze\" a layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Pdb\n",
    "debugger = Pdb()\n",
    "debugger.set_trace() # put this line as a breakpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen1, gen2 = image_generators(X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cnn_models import batches_generator\n",
    "j = 0\n",
    "x_batches = []\n",
    "y_batches = []\n",
    "\n",
    "for x, y in batches_generator(X[:4], Y[:4], batch_size = 4):\n",
    "    j += 1\n",
    "    if j > 10:\n",
    "        break\n",
    "    x_batches.append(x)\n",
    "    y_batches.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(x_batches).shape, np.array(y_batches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = 0\n",
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i += 1\n",
    "if i >= x_batches[0].shape[0]:\n",
    "    i = 0\n",
    "    b += 1\n",
    "print(\"Batch\", str(b) + \". Image\", i)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches((20, 10))\n",
    "axs[0].imshow(x_batches[b][i], cmap='gray')\n",
    "axs[1].imshow(y_batches[b][i][:, :, 1], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
