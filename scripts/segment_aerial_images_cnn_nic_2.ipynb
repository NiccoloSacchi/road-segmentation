{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check here\n",
    "\n",
    "[https://github.com/anujshah1003/own_data_cnn_implementation_keras/blob/master/custom_data_cnn.py]\n",
    "\n",
    "[https://www.youtube.com/watch?v=NUuMg5m42-g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from helpers import *\n",
    "\n",
    "# import 'Sequential' is a linear stack of neural network layers. Will be used to build the feed-forward CNN\n",
    "from keras.models import Sequential \n",
    "# import the \"core\" layers from Keras (these are the most common layers)\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# import the convolutional layers that will help us efficiently train on image data\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# these utilities will help us transform our data\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The goal here is to use the CNN to reduce the size of the input image to obtain a \"discretized\" image of shape (W/16, H/16). Every entry of this image is related to a patch in the input image. This obtained image is compared by the CNN with the groundtruth, after discretizing by \"discretizing\" it patch-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded a set of images\n",
    "n = 20\n",
    "\n",
    "imgs, gt_imgs = load_images(n)\n",
    "\n",
    "imgs[0].shape, gt_imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - For now avoid cross validation, just split the datasest in test and train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "train, test = split_train_test(imgs, gt_imgs, train_ratio=train_ratio, seed=1)\n",
    "train.imgs.shape, train.gt_imgs.shape, test.imgs.shape, test.gt_imgs.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Generate the inpus and the outputs\n",
    "We reshape each input to fulfill the requirements of the tensorflow library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.gt_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = gt_imgs_to_Y(train.gt_imgs, 8)\n",
    "asd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_imgs_to_Y(gt_imgs, patch_width=8):\n",
    "    gt_matr = np.array([img_crop_matr(gt_img, patch_width) for gt_img in gt_imgs])\n",
    "    Y = np.zeros((gt_matr.shape[0], gt_matr.shape[1], gt_matr.shape[2])) # keep width and heigth but convert the patch to a class\n",
    "    for im in range(Y.shape[0]):\n",
    "        for i in range(Y.shape[1]):\n",
    "            for j in range(Y.shape[2]):\n",
    "                Y[im, i, j] = value_to_class(np.mean(gt_matr[im, i, j]))\n",
    "    return np_utils.to_categorical(Y, 2)\n",
    "    \n",
    "train.X = train.imgs\n",
    "train.Y = gt_imgs_to_Y(train.gt_imgs, 8)\n",
    "\n",
    "test.X = test.imgs\n",
    "test.Y = gt_imgs_to_Y(test.gt_imgs, 8)\n",
    "train.X.shape, train.Y.shape, test.X.shape, test.Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap on **train** (and test) object:\n",
    "- **train.imgs**: (expanded) images of the training set. **shape**=(#train images, 400, 400, 3)\n",
    "- **train.gt_imgs**: respective groundtruth images. **shape**=(#train images, 400, 400)\n",
    "- **train.X**: inputs extracted from train.imgs. **shape**=(#total windows, 72, 72, 3)\n",
    "- **train.Y**: outputs extracted from train.gt_imgs. **shape**=(#total windows, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Build the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to build the cnn model:\n",
    "- **Convolutional layers**: Conv1D or **Conv2D** (1D: covolution over a single spatial dimension)\n",
    "    - **filters**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = 2\n",
    "model = Sequential()\n",
    "\n",
    "# padding=\"same\" => output width/heigth = input width/heigth\n",
    "\n",
    "# layer 1\n",
    "model.add(\n",
    "    Conv2D(32, (11, 11), \n",
    "           activation='relu', \n",
    "           padding=\"same\", \n",
    "           input_shape=train.X[0].shape)) # variable size input?\n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# layer 2\n",
    "model.add(\n",
    "    Conv2D(48, (5, 5),\n",
    "           strides=(2, 2),\n",
    "           activation='relu',\n",
    "           padding=\"same\"\n",
    "          ))\n",
    "\n",
    "# later 3\n",
    "model.add(\n",
    "    Conv2D(48, (5, 5), \n",
    "           activation='relu', \n",
    "           padding=\"same\")) # variable size input\n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# layer 4\n",
    "model.add(\n",
    "    Conv2D(48, (5, 5),\n",
    "           strides=(2, 2),\n",
    "           activation='relu',\n",
    "           padding=\"same\"))\n",
    "\n",
    "# layer 5\n",
    "model.add(\n",
    "    Conv2D(48, (5, 5), \n",
    "           activation='relu', \n",
    "           padding=\"same\")) # variable size input\n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# layer 6\n",
    "model.add(\n",
    "    Conv2D(64, (5, 5), \n",
    "           strides=(2, 2),\n",
    "           activation='relu', \n",
    "           padding=\"same\")) # variable size input\n",
    "\n",
    "# layer 7\n",
    "model.add(\n",
    "    Conv2D(64, (5, 5), \n",
    "           activation='relu', \n",
    "           padding=\"same\")) # variable size input\n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# layer 8\n",
    "model.add(\n",
    "    Conv2D(2, (5, 5), \n",
    "           activation='relu', \n",
    "           padding=\"same\")) # variable size input\n",
    "\n",
    "\n",
    "model.add(Activation(K.softmax))\n",
    "\n",
    "train.X[0].shape, model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fit the model on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.X.shape, train.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either validation_data=(test.X, test.Y) or validation_split=0.2\n",
    "hist=model.fit(train.X, train.Y, batch_size=32, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing losses and accuracy\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "xc=range(num_epoch)\n",
    "\n",
    "plt.figure(1,figsize=(7,5))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'])\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "plt.figure(2,figsize=(7,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,val_acc)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'],loc=4)\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the intermediate layer\n",
    "\n",
    "#\n",
    "def get_featuremaps(model, layer_idx, X_batch):\n",
    "#     visualize layer n layer_idx\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()],[model.layers[layer_idx].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "layer_num=3\n",
    "filter_num=0\n",
    "\n",
    "activations = get_featuremaps(model, int(layer_num),test_image)\n",
    "\n",
    "print (np.shape(activations))\n",
    "feature_maps = activations[0][0]      \n",
    "print (np.shape(feature_maps))\n",
    "\n",
    "if K.image_dim_ordering()=='th':\n",
    "    feature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\n",
    "print (feature_maps.shape)\n",
    "\n",
    "# show just one filter relative to the layer\n",
    "fig=plt.figure(figsize=(16,16))\n",
    "plt.imshow(feature_maps[:,:,filter_num],cmap='gray')\n",
    "plt.savefig(\"featuremaps-layer-{}\".format(layer_num) + \"-filternum-{}\".format(filter_num)+'.jpg')\n",
    "\n",
    "# show all the filters\n",
    "num_of_featuremaps=feature_maps.shape[2]\n",
    "fig=plt.figure(figsize=(16,16))\t\n",
    "plt.title(\"featuremaps-layer-{}\".format(layer_num))\n",
    "subplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\n",
    "for i in range(int(num_of_featuremaps)):\n",
    "    ax = fig.add_subplot(subplot_num, subplot_num, i+1)\n",
    "    #ax.imshow(output_image[0,:,:,i],interpolation='nearest' ) #to see the first filter\n",
    "    ax.imshow(feature_maps[:,:,i],cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"featuremaps-layer-{}\".format(layer_num) + '.jpg')\n",
    "# you should be able to see that the cnn is keeping les information while the image goes through (keep only the\n",
    "# important features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(test.X, test.Y, verbose=1)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict_classes(test.X, verbose=1)\n",
    "# stats(Z, test.Y[:, 1])\n",
    "Z.shape, test.Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "display_ith_prediction(test, Z, i, window_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(y_pred)\n",
    "#y_pred = model.predict_classes(X_test)\n",
    "#print(y_pred)\n",
    "target_names = ['class 0(cats)', 'class 1(Dogs)', 'class 2(Horses)','class 3(Humans)']\n",
    "\n",
    "# print precision, recall, F1 score\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix')\n",
    "#plt.figure()\n",
    "# Plot normalized confusion matrix\n",
    "#plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "#plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**problem**: it predicts always the same class [check here](https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading model and weights\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model.save('model.hdf5')\n",
    "loaded_model=load_model('model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
